---
title: "Modelling"
author: "Yixin Zheng (yz4993), Thomas Tang (tt3022), Yonghao YU (yy3564)"
date: "2024-11-16"
output: github_document
---

## Yixin Zheng (yz4993), Thomas Tang (tt3022), Yonghao YU (yy3564)
```{r setup}
library(tidyverse)
library(janitor)
```

```{r import data author: Yixin Zheng, Stella Koo, include = FALSE, eval=FALSE}
save_path <- "./data"

column_names <- c("age", "sex", "cp", "trestbps", "chol", "fbs", 
                  "restecg", "thalach", "exang", "oldpeak", "slope", 
                  "ca", "thal", "num")

process_data <- function(file_path, save_name) {

    data <- read.table(file_path, sep = ",", header = FALSE)
    
    colnames(data) <- column_names
    
    write.csv(data, file.path(save_path, save_name), row.names = FALSE)
}

process_data("./heart+disease/processed.cleveland.data", "cleveland.csv")
process_data("./heart+disease/processed.hungarian.data", "hungarian.csv")
process_data("./heart+disease/processed.va.data", "long_beach_va.csv")
process_data("./heart+disease/processed.switzerland.data", "switzerland.csv")
```


Interpreting num (author: Yixin Zheng):
The values for `num` represent the degree of narrowing in the coronary arteries:
0: No disease (< 50% diameter narrowing).
1-4: Increasing severity of disease (> 50% diameter narrowing, with different severities).

For convenience, this variable will binarized:
0: No heart disease (value 0 in num).
1: Presence of heart disease (values 1-4 in num).

*but if we want to analyze the severity of heart disease num will be treated as a categorical variable.
example code:
cleaned_data <- data |> 
  mutate(num = factor(num, levels = c(0, 1, 2, 3, 4), 
                      labels = c("No Disease", "Mild", "Moderate", "Severe", "Very Severe")))
                      
```{r data cleaning author: Yixin Zheng}
cleveland <- read_csv("./data/cleveland.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1)) # Binarize the `num` variable: 0 = no heart disease, 1 = heart disease

# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

hungary = read_csv("./data/hungarian.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

long_beach = read_csv("./data/long_beach_va.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

switzerland = read_csv("./data/switzerland.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed) 

```

```{r}
cor(cleveland$chol, cleveland$num, use = "complete.obs")
```

# Variable Selection (author: Yonghao YU)

author: Yonghao YU

### Data Preprocessing

```{r}
cleveland$region = "Cleveland"
hungary$region = "Hungarian"
long_beach$region = "Long_Beach_VA"
switzerland$region = "Switzerland"
combined_data_one = bind_rows(cleveland, hungary, long_beach, switzerland)

colnames(combined_data_one) = c("age", "sex", "cp", "trestbps", "chol", "fbs",
                             "restecg", "thalach", "exang", "oldpeak", "slope",
                             "ca", "thal", "num", "region")
combined_data_two = combined_data_one |>
  mutate(region = case_when(
    region == "Cleveland" ~ 1,
    region == "Hungarian" ~ 2,
    region == "Long_Beach_VA" ~ 3,
    region == "Switzerland" ~ 4,
  )) |>
  select(-thal,-ca) |>
  drop_na()
case_data = combined_data_two |>
  filter(num == 1)
control_data = combined_data_two |>
  filter(num == 0)
print(case_data)
print(control_data)
print(combined_data_two)
```


author: Yonghao YU

### For Continues case
For continuous variables, we use mean and standard deviation (std) to describe the distribution in overall samples, samples of control(num = 0), and samples of case(num = 1). Then, we use t-test to examine whether the means of these variables are significantly different between case group and control group (significance level = 0.05).

```{r}
# 1. Mean and Std for Continuous Variables (Overall)
list_conti_all = list(
  age = combined_data_two$age,
  trestbps = combined_data_two$trestbps,
  chol = combined_data_two$chol,
  thalach = combined_data_two$thalach,
  oldpeak = combined_data_two$oldpeak
) |> 
  lapply(na.omit) 

mean_all = sapply(list_conti_all, mean) |> 
  as.data.frame()|>
  setNames("Overall Mean")

std_all = sapply(list_conti_all, sd) |> 
  as.data.frame() |>
  setNames("Overall Std")

# 2. p-value of t-test for Continuous Variables
t_test = function(variable) {
  t_test_result = t.test(combined_data_two[[variable]] ~ combined_data_two$num)
  return(data.frame(
    variable = variable,
    p_value = t_test_result$p.value
  ))
}

p_value = 
  lapply(c("age", "trestbps", "chol", "thalach", "oldpeak"), t_test) |> 
  bind_rows() |> 
  as.data.frame()

# 3. Mean and Std for Control Group
list_conti_control = list(
  age = control_data$age,
  trestbps = control_data$trestbps,
  chol = control_data$chol,
  thalach = control_data$thalach,
  oldpeak = control_data$oldpeak
) |> 
  lapply(na.omit)

mean_control = sapply(list_conti_control, mean) |> 
  as.data.frame() |>
  setNames("Control Mean")

std_control = sapply(list_conti_control, sd) |> 
  as.data.frame() |>
  setNames("Control Std")

# 4. Mean and Std for Case Group
list_conti_case = list(
  age = case_data$age,
  trestbps = case_data$trestbps,
  chol = case_data$chol,
  thalach = case_data$thalach,
  oldpeak = case_data$oldpeak
) |> 
  lapply(na.omit)

mean_case = sapply(list_conti_case, mean) |> 
  as.data.frame() |>
  setNames("Case Mean")

std_case = sapply(list_conti_case, sd) |> 
  as.data.frame() |>
  setNames("Case Std")

conti_des_df =
  as.data.frame(cbind(mean_all, std_all, mean_control, std_control, mean_case, std_case, p_value))
conti_des_df = conti_des_df[, -grep("variable", colnames(conti_des_df))] |> 
  knitr::kable(digits = 6)
conti_des_df
```

Based on the result, we can find that all five features are significantly different between case and control.


author: Yonghao YU

### For Discrete case
For binary and categorical variables, we use count (n) and percentage (pct) to describe the distribution in overall samples, samples of control(num = 0), and samples of case(num = 1). Then, as the data meet the assumption, we use chi-sq test to examine whether the distribution of these variables are significantly different between case group and control group (significance level = 0.05).

```{r}
list_cat_all = as.data.frame(list(
  sex = combined_data_two$sex,
  cp = combined_data_two$cp,
  fbs = combined_data_two$fbs,
  restecg = combined_data_two$restecg,
  exang = combined_data_two$exang,
  slope = combined_data_two$slope,
  region = combined_data_two$region
))

# 1. Overall Counts and Chi-Square Test
cat_vars = names(list_cat_all)

count_all_function = function(variable) {
  table_value = table(list_cat_all[[variable]], combined_data_two$num) 
  chi_sq_test = chisq.test(table_value)
  
  count = table(list_cat_all[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    n = as.numeric(count),
    pct = round(pct, 3),
    p_value = round(chi_sq_test$p.value, 3)
  )
  
  return(result_df)
}

cat_count_chisq = lapply(cat_vars, count_all_function) |> 
  bind_rows()

# 2. Control Group Counts and Percentages
list_cat_ctrl = as.data.frame(list(
  sex = control_data$sex,
  cp = control_data$cp,
  fbs = control_data$fbs,
  restecg = control_data$restecg,
  exang = control_data$exang,
  slope = control_data$slope,
  region = control_data$region
))

cat_vars_ctrl = names(list_cat_ctrl)

count_ctrl_function = function(variable) {
  count = table(list_cat_ctrl[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    control_n = as.numeric(count),
    control_pct = round(pct, 3)
  )
  
  return(result_df)
}

cat_count_ctrl = lapply(cat_vars_ctrl, count_ctrl_function) |> 
  bind_rows()

# 3. Case Group Counts and Percentages
list_cat_case = as.data.frame(list(
  sex = case_data$sex,
  cp = case_data$cp,
  fbs = case_data$fbs,
  restecg = case_data$restecg,
  exang = case_data$exang,
  slope = case_data$slope,
  region = case_data$region
))

cat_vars_case = names(list_cat_case)

count_case_function = function(variable) {
  count = table(list_cat_case[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    case_n = as.numeric(count),
    case_pct = round(pct, 3)
  )
  return(result_df)
}

cat_count_case = lapply(cat_vars_case, count_case_function) |> 
  bind_rows()

# 4. Combine Results
final_cat_count = cat_count_chisq |>
  left_join(cat_count_ctrl, by = c("variable", "category")) |>
  left_join(cat_count_case, by = c("variable", "category"))|>
  knitr::kable(digits = 3)

print(final_cat_count)
```

Based on the result, we can find that except fbs, the rest of all other binary and categorical features are significantly different between case and control.




hypothesis (author: Yixin Zheng)
* need to run some test choose explanatory variables?

1. Comparing Diagnostic Factors for Heart Disease Across Regions
 explore whether certain diagnostic factors (e.g., cholesterol levels, exercise-induced angina) are more predictive of heart disease in one region compared to others.
 
* there are many null values in the `chol` column in switzerland dataset. So maybe we need to use other diagnostic factors such as `trestbps` if we want to compare the regions
Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between cholesterol levels (`chol`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a positive linear relationship between cholesterol levels (`chol`) and the presence of heart disease (`num`). ($\beta_1>0$)
  Where:
  - `num` is the target variable indicating heart disease presence.
  - `chol` is the predictor variable representing cholesterol levels.

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \epsilon $$

Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Cholesterol levels (`chol`), blood pressure (`trestbps`), and exercise-induced angina (`exang`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`chol`, `trestbps`, `exang`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \beta_2 \cdot \text{trestbps} + \beta_3 \cdot \text{exang} + \epsilon $$

2. Examining Predictive Power of Clinical Indicators for Heart Disease in Diverse Populations

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between maximum heart rate achieved (`thalach`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a negative linear relationship between maximum heart rate achieved (`thalach`) and the presence of heart disease (`num`). ($\beta_1<0$)

$$ num = \beta_0 + \beta_1 \cdot \text{thalach} + \epsilon $$
Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Maximum heart rate (`thalach`), fasting blood sugar (`fbs`), and the slope of the ST segment (`slope`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`thalach`, `fbs`, `slope`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{thalach} + \beta_2 \cdot \text{fbs} + \beta_3 \cdot \text{slope} + \epsilon $$

3. Influence of Age and Lifestyle Factors on Heart Disease

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between age (`age`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a positive linear relationship between age (`age`) and the presence of heart disease (`num`). ($\beta_1>0$)

$$ num = \beta_0 + \beta_1 \cdot \text{age} + \epsilon $$
Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Age (`age`), chest pain type (`cp`), and exercise-induced angina (`exang`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`age`, `cp`, `exang`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{age} + \beta_2 \cdot \text{cp} + \beta_3 \cdot \text{exang} + \epsilon $$

4. Regional Patterns in Heart Disease Diagnostic Attributes

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between diagnostic attributes (e.g., cholesterol, age) and heart disease (`num`) in each region.
- **Alternative Hypothesis (H_a):**The relationship between diagnostic attributes (e.g., cholesterol, age) and heart disease (`num`) differs significantly across regions.
- This hypothesis can be tested by performing SLR for each region and comparing the coefficients (beta_1 values) to see if they vary.

Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Regional differences do not significantly influence the predictive power of diagnostic factors for heart disease.
- **Alternative Hypothesis (H_a):** Regional differences significantly influence the predictive power of diagnostic factors for heart disease.

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \beta_2 \cdot \text{age} + \beta_3 \cdot \text{region} + \beta_4 \cdot (\text{chol} \times \text{region}) + \beta_5 \cdot (\text{age} \times \text{region}) + \epsilon $$


author: Thomas Tang
```{r }
cleveland <- read.csv("./data/cleveland.csv", header = FALSE)
hungarian <- read.csv("./data/hungarian.csv", header = FALSE)
long_beach <- read.csv("./data/long_beach_va.csv", header = FALSE)
switzerland <- read.csv("./data/switzerland.csv", header = FALSE)

# Add region column and combine datasets
cleveland$region <- "Cleveland"
hungarian$region <- "Hungarian"
long_beach$region <- "Long_Beach_VA"
switzerland$region <- "Switzerland"
combined_data <- bind_rows(cleveland, hungarian, long_beach, switzerland)

colnames(combined_data) <- c("age", "sex", "cp", "trestbps", "chol", "fbs",
                             "restecg", "thalach", "exang", "oldpeak", "slope",
                             "ca", "thal", "num", "region")
combined_data <- combined_data %>%
  mutate(across(c(age, sex, cp, trestbps, chol, fbs, restecg, thalach,
                  exang, oldpeak, slope, ca, thal, num), as.numeric))
```
Dropped variables with excessive missing values (ca and thal).
Removed rows with missing values in critical variables.
Converted num to binary (0 = no heart disease, 1 = heart disease) and set as a factor.
```{R}
cleaned_data <- combined_data %>% select(-ca, -thal)
critical_columns <- c("num", "age", "sex", "cp", "trestbps", "chol",
                      "fbs", "restecg", "thalach", "exang", "oldpeak", "slope")
cleaned_data <- cleaned_data %>% drop_na(all_of(critical_columns))
summary(cleaned_data)
```
```{r author: Thomas Tang}
cleaned_data$num <- ifelse(cleaned_data$num > 0, 1, 0)
cleaned_data$num <- as.factor(cleaned_data$num)
logistic_model <- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                      restecg + thalach + exang + oldpeak,
                      data = cleaned_data, family = binomial)
summary(logistic_model)
```
(author= Thomas Tang)
Significant Predictors (p-value < 0.05):
sex (1.409350):
Males are significantly more likely to have heart disease than females.
Odds ratio: exp(1.409)=4.09 → Males are 4.09 times more likely to have heart disease compared to females.
cp (0.687260):
Higher chest pain levels increase the odds of heart disease.
Odds ratio: exp(0.687)=1.99.
trestbps (0.013883):
Resting blood pressure has a small but significant positive effect.
Odds ratio: exp(0.0139)=1.014 per unit increase.
chol (-0.003090):
Cholesterol has a small negative association.
Odds ratio: exp(−0.0031)=0.997, slightly decreasing odds.
thalach (-0.021518):
Higher maximum heart rate achieved decreases the odds of heart disease.
Odds ratio: exp(−0.0215)=0.978 → Protective effect.
exang (0.894064):
Exercise-induced angina increases the odds of heart disease. exp(0.894)=2.44.
oldpeak (0.586433):Higher ST depression values significantly increase the odds of heart disease.
Odds ratio: exp(0.586)=1.80.
fbs (Fasting blood sugar) and age do not have significant association with heart disease in this dataset.

AIC: 469.38 (used for model comparison).

```{r}
male_data <- cleaned_data %>% filter(sex == 1)
female_data <- cleaned_data %>% filter(sex == 0)

male_model <- glm(num ~ age + cp + trestbps + chol + fbs + restecg +
                  thalach + exang + oldpeak, data = male_data, family = binomial)
female_model <- glm(num ~ age + cp + trestbps + chol + fbs + restecg +
                    thalach + exang + oldpeak, data = female_data, family = binomial)
male_results <- summary(male_model)$coefficients
female_results <- summary(female_model)$coefficients

results_table <- data.frame(
  Variable = rownames(male_results),
  Male_Estimate = male_results[, "Estimate"],
  Male_Std_Error = male_results[, "Std. Error"],
  Male_P_Value = male_results[, "Pr(>|z|)"],
  Female_Estimate = female_results[, "Estimate"],
  Female_Std_Error = female_results[, "Std. Error"],
  Female_P_Value = female_results[, "Pr(>|z|)"]
)
print(results_table)
```

Significant Predictors for Males:
Chest Pain (cp): Strong positive effect (β=0.62, p<0.001).
Resting Blood Pressure (trestbps): Positive and weakly significant (β=0.007, p=0.039).
Max Heart Rate (thalach): Negative and significant (β=−0.026, p=0.001), indicating a protective effect.
Exercise-Induced Angina (exang): Positive and significant (β=0.71, p=0.025).
ST Depression (oldpeak): Strong positive effect (β=0.57, p<0.001).

Significant Predictors for Females:
Chest Pain (cp): Stronger positive effect than males (β=0.93, p=0.006).
Resting Blood Pressure (trestbps): Positive and significant (β=0.033, p=0.023).
Exercise-Induced Angina (exang): Positive and significant (β=1.14, p=0.037), stronger effect than in males.
ST Depression (oldpeak): Strong positive effect (β=0.68, p=0.020).

Key Comparisons Between Genders
Chest Pain (cp): Stronger predictor for females (β=0.93) than males (β=0.62).
Exercise-Induced Angina (exang): Higher odds for females (β=1.14) than males (β=0.71).
ST Depression (oldpeak): Significant and strong for both genders, slightly higher in females (β=0.68).





(author: Yonghao YU)

## Try Random Forest Classifier!

### A brief intro to Random Forest Algorithm

Random Forest is an ensemble learning algorithm used for classification and regression tasks. It builds multiple decision trees using bootstrap sampling (random subsets of data) and selects features randomly at each split to increase diversity.   Each tree predicts independently, and the final output is determined by majority voting (classification) or averaging (regression).   Random Forest is robust to overfitting, handles high-dimensional data well, and provides feature importance scores.


### First, construct the model and generate the results! 
author: Yonghao YU
```{r}
library(caret)
library(randomForest)

# drop out the variable "ca" and "thal" which are have so many missing values inside
variables = c("chol", "cp", "age", "thalach", "oldpeak", "num", "restecg", "fbs", "region", "slope", "trestbps", "exang")
data = combined_data_two[, variables]
data$num = as.factor(data$num)

# check and deal with missing data
if (any(is.na(data))) {
  print("Missing value detected")
  data = na.omit(data)
  print("Missing data have been deleted")
}
# split the dataset into training and testing datasets
set.seed(42)
trainIndex = createDataPartition(data$num, p = 0.8, list = FALSE)
trainData = data[trainIndex, ]
testData = data[-trainIndex, ]

# Construct the random forest model and evaluate the model results
rf_model = randomForest(num ~ ., data = trainData, importance = TRUE)
rf_pred = predict(rf_model, testData)
rf_conf_matrix = confusionMatrix(rf_pred, testData$num)
print("The model result is")
print(rf_conf_matrix)
```

From the model we can observe the following things: 
1. The model correctly classified 77.14% of the instances.
2. The true accuracy is expected to fall 95% of the time in (0.6793, 0.8477)
3. The information rate is 0.6095 which is less than the accuracy rate (p-value also indicate this), indicating the model we built actually capture some significant features.
4. The Kappa is 0.5281 which is in the range [40,60], which indicate our classifier achieves moderate level of classification
5. High sensitivity (0.7561) indicates good identification of positives.
6. High specificity (0.7812) indicates good identification of negatives.
7. Balanced accuracy (76.87%) suggests the model balances its performance across both classes well.

author: Yonghao YU
# Then we show the feature importance trends(The trend is descending according to the MeanDecreaseAccuracy)
```{r,fig.width=10, fig.height=8}

var_imp = importance(rf_model)
var_imp_df = as.data.frame(var_imp)
var_imp_df$Variable = rownames(var_imp_df)
rownames(var_imp_df) = NULL
var_imp_df = var_imp_df[order(var_imp_df$MeanDecreaseAccuracy, decreasing = TRUE), ]
ggplot(var_imp_df, aes(x = reorder(Variable, -MeanDecreaseAccuracy))) +
  geom_line(aes(y = MeanDecreaseAccuracy, group = 1, color = "MeanDecreaseAccuracy")) +
  geom_point(aes(y = MeanDecreaseAccuracy, color = "MeanDecreaseAccuracy")) +
  geom_line(aes(y = MeanDecreaseGini, group = 1, color = "MeanDecreaseGini")) +
  geom_point(aes(y = MeanDecreaseGini, color = "MeanDecreaseGini")) +
  labs(title = "Feature Importance Trends",
       x = "Features",
       y = "Importance",
       color = "Metric") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1))
```  


Then we ranked the predictors descendingly based on the MeanDecreaseAccuracy which measures the decrease in overall model accuracy when the variable is permuted. And we show them in a line plot! Based on the MeanDecreaseAccuracy and MeanDecreaseGini, we can drop out restecg, trestbps, and fbs predictors that have relatively small impact on our prediction. And then we can focus on the first eight predictors that have more impact on our prediction results!

### Last, simulate the prediction which predicts the num with the new data based on the model we built!

author: Yonghao YU
```{r}
# construct a new dataframe which includes new data
new_data = data.frame(
  age = c(63,39),
  sex = c(1, 1),
  cp = c(1, 2),
  trestbps = c(145, 120),
  chol = c(233, 200),
  fbs = c(1, 0),
  restecg = c(2, 0),
  thalach = c(150, 160),
  exang = c(0, 1),
  oldpeak = c(2.3, 1),
  slope = c(3, 2),
  region = c(1, 2)
)
# predict the results based on the model we have trained
predicted_num = predict(rf_model, new_data)
print("The prediction result is：")
print(data.frame(new_data, Predicted_num = predicted_num))
```

From the results, we can see that the model can generate some results based on the predictor values we put in!
