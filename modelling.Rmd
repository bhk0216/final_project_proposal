---
title: "Modelling"
author: "Yixin Zheng (yz4993), Thomas Tang (tt3022), Yonghao YU (yy3564)"
date: "2024-11-16"
output: github_document
---

## Yixin Zheng (yz4993), Thomas Tang (tt3022), Yonghao YU (yy3564)
```{r setup}
library(tidyverse)
library(janitor)
```

```{r import data author: Yixin Zheng, Stella Koo, include = FALSE, eval=FALSE}
save_path <- "./data"

column_names <- c("age", "sex", "cp", "trestbps", "chol", "fbs", 
                  "restecg", "thalach", "exang", "oldpeak", "slope", 
                  "ca", "thal", "num")

process_data <- function(file_path, save_name) {

    data <- read.table(file_path, sep = ",", header = FALSE)
    
    colnames(data) <- column_names
    
    write.csv(data, file.path(save_path, save_name), row.names = FALSE)
}

process_data("./heart+disease/processed.cleveland.data", "cleveland.csv")
process_data("./heart+disease/processed.hungarian.data", "hungarian.csv")
process_data("./heart+disease/processed.va.data", "long_beach_va.csv")
process_data("./heart+disease/processed.switzerland.data", "switzerland.csv")
```


Interpreting num (author: Yixin Zheng):
The values for `num` represent the degree of narrowing in the coronary arteries:
0: No disease (< 50% diameter narrowing).
1-4: Increasing severity of disease (> 50% diameter narrowing, with different severities).

For convenience, this variable will binarized:
0: No heart disease (value 0 in num).
1: Presence of heart disease (values 1-4 in num).

*but if we want to analyze the severity of heart disease num will be treated as a categorical variable.
example code:
cleaned_data <- data |> 
  mutate(num = factor(num, levels = c(0, 1, 2, 3, 4), 
                      labels = c("No Disease", "Mild", "Moderate", "Severe", "Very Severe")))
                      
```{r data cleaning author: Yixin Zheng}
cleveland <- read_csv("./data/cleveland.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1)) # Binarize the `num` variable: 0 = no heart disease, 1 = heart disease

# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

hungary = read_csv("./data/hungarian.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

long_beach = read_csv("./data/long_beach_va.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

switzerland = read_csv("./data/switzerland.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed) 

```

```{r}
cor(cleveland$chol, cleveland$num, use = "complete.obs")
```

# Variable Selection (author: Yonghao YU)

author: Yonghao YU

### Data Preprocessing

```{r}
cleveland$region = "Cleveland"
hungary$region = "Hungarian"
long_beach$region = "Long_Beach_VA"
switzerland$region = "Switzerland"
combined_data_one = bind_rows(cleveland, hungary, long_beach, switzerland)

colnames(combined_data_one) = c("age", "sex", "cp", "trestbps", "chol", "fbs",
                             "restecg", "thalach", "exang", "oldpeak", "slope",
                             "ca", "thal", "num", "region")
combined_data_two = combined_data_one |>
  mutate(region = case_when(
    region == "Cleveland" ~ 1,
    region == "Hungarian" ~ 2,
    region == "Long_Beach_VA" ~ 3,
    region == "Switzerland" ~ 4,
  )) |>
  select(-thal,-ca) |>
  drop_na()
case_data = combined_data_two |>
  filter(num == 1)
control_data = combined_data_two |>
  filter(num == 0)
print(case_data)
print(control_data)
print(combined_data_two)
```


author: Yonghao YU

### For Continues case
For continuous variables, we use mean and standard deviation (std) to describe the distribution in overall samples, samples of control(num = 0), and samples of case(num = 1). Then, we use t-test to examine whether the means of these variables are significantly different between case group and control group (significance level = 0.05).

```{r}
# 1. Mean and Std for Continuous Variables (Overall)
list_conti_all = list(
  age = combined_data_two$age,
  trestbps = combined_data_two$trestbps,
  chol = combined_data_two$chol,
  thalach = combined_data_two$thalach,
  oldpeak = combined_data_two$oldpeak
) |> 
  lapply(na.omit) 

mean_all = sapply(list_conti_all, mean) |> 
  as.data.frame()|>
  setNames("Overall Mean")

std_all = sapply(list_conti_all, sd) |> 
  as.data.frame() |>
  setNames("Overall Std")

# 2. p-value of t-test for Continuous Variables
t_test = function(variable) {
  t_test_result = t.test(combined_data_two[[variable]] ~ combined_data_two$num)
  return(data.frame(
    variable = variable,
    p_value = t_test_result$p.value
  ))
}

p_value = 
  lapply(c("age", "trestbps", "chol", "thalach", "oldpeak"), t_test) |> 
  bind_rows() |> 
  as.data.frame()

# 3. Mean and Std for Control Group
list_conti_control = list(
  age = control_data$age,
  trestbps = control_data$trestbps,
  chol = control_data$chol,
  thalach = control_data$thalach,
  oldpeak = control_data$oldpeak
) |> 
  lapply(na.omit)

mean_control = sapply(list_conti_control, mean) |> 
  as.data.frame() |>
  setNames("Control Mean")

std_control = sapply(list_conti_control, sd) |> 
  as.data.frame() |>
  setNames("Control Std")

# 4. Mean and Std for Case Group
list_conti_case = list(
  age = case_data$age,
  trestbps = case_data$trestbps,
  chol = case_data$chol,
  thalach = case_data$thalach,
  oldpeak = case_data$oldpeak
) |> 
  lapply(na.omit)

mean_case = sapply(list_conti_case, mean) |> 
  as.data.frame() |>
  setNames("Case Mean")

std_case = sapply(list_conti_case, sd) |> 
  as.data.frame() |>
  setNames("Case Std")

conti_des_df =
  as.data.frame(cbind(mean_all, std_all, mean_control, std_control, mean_case, std_case, p_value))
conti_des_df = conti_des_df[, -grep("variable", colnames(conti_des_df))] |> 
  knitr::kable(digits = 6)
conti_des_df
```

Based on the result, we can find that all five features are significantly different between case and control.


author: Yonghao YU

### For Discrete case
For binary and categorical variables, we use count (n) and percentage (pct) to describe the distribution in overall samples, samples of control(num = 0), and samples of case(num = 1). Then, as the data meet the assumption, we use chi-sq test to examine whether the distribution of these variables are significantly different between case group and control group (significance level = 0.05).

```{r}
list_cat_all = as.data.frame(list(
  sex = combined_data_two$sex,
  cp = combined_data_two$cp,
  fbs = combined_data_two$fbs,
  restecg = combined_data_two$restecg,
  exang = combined_data_two$exang,
  slope = combined_data_two$slope,
  region = combined_data_two$region
))

# 1. Overall Counts and Chi-Square Test
cat_vars = names(list_cat_all)

count_all_function = function(variable) {
  table_value = table(list_cat_all[[variable]], combined_data_two$num) 
  chi_sq_test = chisq.test(table_value)
  
  count = table(list_cat_all[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    n = as.numeric(count),
    pct = round(pct, 3),
    p_value = round(chi_sq_test$p.value, 3)
  )
  
  return(result_df)
}

cat_count_chisq = lapply(cat_vars, count_all_function) |> 
  bind_rows()

# 2. Control Group Counts and Percentages
list_cat_ctrl = as.data.frame(list(
  sex = control_data$sex,
  cp = control_data$cp,
  fbs = control_data$fbs,
  restecg = control_data$restecg,
  exang = control_data$exang,
  slope = control_data$slope,
  region = control_data$region
))

cat_vars_ctrl = names(list_cat_ctrl)

count_ctrl_function = function(variable) {
  count = table(list_cat_ctrl[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    control_n = as.numeric(count),
    control_pct = round(pct, 3)
  )
  
  return(result_df)
}

cat_count_ctrl = lapply(cat_vars_ctrl, count_ctrl_function) |> 
  bind_rows()

# 3. Case Group Counts and Percentages
list_cat_case = as.data.frame(list(
  sex = case_data$sex,
  cp = case_data$cp,
  fbs = case_data$fbs,
  restecg = case_data$restecg,
  exang = case_data$exang,
  slope = case_data$slope,
  region = case_data$region
))

cat_vars_case = names(list_cat_case)

count_case_function = function(variable) {
  count = table(list_cat_case[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    case_n = as.numeric(count),
    case_pct = round(pct, 3)
  )
  return(result_df)
}

cat_count_case = lapply(cat_vars_case, count_case_function) |> 
  bind_rows()

# 4. Combine Results
final_cat_count = cat_count_chisq |>
  left_join(cat_count_ctrl, by = c("variable", "category")) |>
  left_join(cat_count_case, by = c("variable", "category"))|>
  knitr::kable(digits = 3)

print(final_cat_count)
```

Based on the result, we can find that except fbs, the rest of all other binary and categorical features are significantly different between case and control.




hypothesis (author: Yixin Zheng)
* need to run some test choose explanatory variables?

1. Comparing Diagnostic Factors for Heart Disease Across Regions
 explore whether certain diagnostic factors (e.g., cholesterol levels, exercise-induced angina) are more predictive of heart disease in one region compared to others.
 
* there are many null values in the `chol` column in switzerland dataset. So maybe we need to use other diagnostic factors such as `trestbps` if we want to compare the regions
Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between cholesterol levels (`chol`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a positive linear relationship between cholesterol levels (`chol`) and the presence of heart disease (`num`). ($\beta_1>0$)
  Where:
  - `num` is the target variable indicating heart disease presence.
  - `chol` is the predictor variable representing cholesterol levels.

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \epsilon $$

Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Cholesterol levels (`chol`), blood pressure (`trestbps`), and exercise-induced angina (`exang`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`chol`, `trestbps`, `exang`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \beta_2 \cdot \text{trestbps} + \beta_3 \cdot \text{exang} + \epsilon $$

2. Examining Predictive Power of Clinical Indicators for Heart Disease in Diverse Populations

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between maximum heart rate achieved (`thalach`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a negative linear relationship between maximum heart rate achieved (`thalach`) and the presence of heart disease (`num`). ($\beta_1<0$)

$$ num = \beta_0 + \beta_1 \cdot \text{thalach} + \epsilon $$
Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Maximum heart rate (`thalach`), fasting blood sugar (`fbs`), and the slope of the ST segment (`slope`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`thalach`, `fbs`, `slope`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{thalach} + \beta_2 \cdot \text{fbs} + \beta_3 \cdot \text{slope} + \epsilon $$

3. Influence of Age and Lifestyle Factors on Heart Disease

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between age (`age`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a positive linear relationship between age (`age`) and the presence of heart disease (`num`). ($\beta_1>0$)

$$ num = \beta_0 + \beta_1 \cdot \text{age} + \epsilon $$
Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Age (`age`), chest pain type (`cp`), and exercise-induced angina (`exang`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`age`, `cp`, `exang`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{age} + \beta_2 \cdot \text{cp} + \beta_3 \cdot \text{exang} + \epsilon $$

4. Regional Patterns in Heart Disease Diagnostic Attributes

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between diagnostic attributes (e.g., cholesterol, age) and heart disease (`num`) in each region.
- **Alternative Hypothesis (H_a):**The relationship between diagnostic attributes (e.g., cholesterol, age) and heart disease (`num`) differs significantly across regions.
- This hypothesis can be tested by performing SLR for each region and comparing the coefficients (beta_1 values) to see if they vary.

Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Regional differences do not significantly influence the predictive power of diagnostic factors for heart disease.
- **Alternative Hypothesis (H_a):** Regional differences significantly influence the predictive power of diagnostic factors for heart disease.

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \beta_2 \cdot \text{age} + \beta_3 \cdot \text{region} + \beta_4 \cdot (\text{chol} \times \text{region}) + \beta_5 \cdot (\text{age} \times \text{region}) + \epsilon $$


author: Thomas Tang
```{r }
cleveland <- read.csv("./data/cleveland.csv", header = FALSE)
hungarian <- read.csv("./data/hungarian.csv", header = FALSE)
long_beach <- read.csv("./data/long_beach_va.csv", header = FALSE)
switzerland <- read.csv("./data/switzerland.csv", header = FALSE)

# Add region column and combine datasets
cleveland$region <- "Cleveland"
hungarian$region <- "Hungarian"
long_beach$region <- "Long_Beach_VA"
switzerland$region <- "Switzerland"
combined_data <- bind_rows(cleveland, hungarian, long_beach, switzerland)

colnames(combined_data) <- c("age", "sex", "cp", "trestbps", "chol", "fbs",
                             "restecg", "thalach", "exang", "oldpeak", "slope",
                             "ca", "thal", "num", "region")
combined_data <- combined_data %>%
  mutate(across(c(age, sex, cp, trestbps, chol, fbs, restecg, thalach,
                  exang, oldpeak, slope, ca, thal, num), as.numeric))
```
Dropped variables with excessive missing values (ca and thal).
Removed rows with missing values in critical variables.
Converted num to binary (0 = no heart disease, 1 = heart disease) and set as a factor.
```{R}
cleaned_data <- combined_data %>% select(-ca, -thal)
critical_columns <- c("num", "age", "sex", "cp", "trestbps", "chol",
                      "fbs", "restecg", "thalach", "exang", "oldpeak", 
                      "slope","region")
cleaned_data <- cleaned_data %>% drop_na(all_of(critical_columns))
cleaned_data$num <- ifelse(cleaned_data$num > 0, 1, 0)
cleaned_data$num <- as.factor(cleaned_data$num)
logistic_model <- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                      restecg + thalach + exang + oldpeak + region,
                      data = cleaned_data, family = binomial)
summary(logistic_model)
```
(author= Thomas Tang)
Significant Predictors (p-value < 0.05):
sex (1.409350):
    *Being male increases the log-odds of heart disease significantly.
    *Odds Ratio: exp(1.469)=4.34 → Males are 4.34 times more likely to have heart      disease than females.
    
cp (chest pain):
    *Higher chest pain levels increase the log-odds of heart disease.
    *Odds Ratio: exp(0.668)=1.95 → A strong predictor for heart disease.

thalach (max heart rate achieved):
    *Higher heart rates decrease the log-odds of heart disease.
    *Odds Ratio: exp(−0.014)=0.986, suggesting a protective effect.
exang (exercise-induced angina):
    *Presence of exercise-induced angina increases the odds of heart disease.
    *Odds Ratio: exp(0.899)=2.46.
oldpeak (ST depression):
    *Higher ST depression values significantly increase the odds of heart             disease.
    *Odds Ratio: exp(0.669)=1.95.
regionSwitzerland:
    *Patients from Switzerland have significantly higher odds of heart disease        compared to the reference region (Cleveland).
    *Odds Ratio: exp(3.843)=46.64, suggesting a strong regional effect.

Non-Significant Predictors (p-value > 0.05):
age, trestbps (resting blood pressure), chol (cholesterol), fbs (fasting blood sugar), restecg (resting ECG results), regionHungarian, regionLong_Beach_VA.

Regional Effects:
    *Patients from Switzerland have much higher odds of heart disease compared to      Cleveland, Hungarian and Long Beach VA.

separated by region:
```{r}
cleveland_data <- cleaned_data %>% filter(region == "Cleveland")
hungarian_data <- cleaned_data %>% filter(region == "Hungarian")
long_beach_data <- cleaned_data %>% filter(region == "Long_Beach_VA")
switzerland_data <- cleaned_data %>% filter(region == "Switzerland")

cleveland_model <- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                       restecg + thalach + exang + oldpeak,
                       data = cleveland_data, family = binomial)
hungarian_model <- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                       restecg + thalach + exang + oldpeak,
                       data = hungarian_data, family = binomial)
long_beach_model <- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                        restecg + thalach + exang + oldpeak,
                        data = long_beach_data, family = binomial)
switzerland_model <- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                         restecg + thalach + exang + oldpeak,
                         data = switzerland_data, family = binomial)
extract_results <- function(model, region) {
  coefficients <- summary(model)$coefficients
  data.frame(
    Region = region,
    Variable = rownames(coefficients),
    Estimate = coefficients[, "Estimate"],
    Std_Error = coefficients[, "Std. Error"],
    P_Value = coefficients[, "Pr(>|z|)"]
  )
}

# Extract results for each region
cleveland_results <- extract_results(cleveland_model, "Cleveland")
hungarian_results <- extract_results(hungarian_model, "Hungarian")
long_beach_results <- extract_results(long_beach_model, "Long_Beach_VA")
switzerland_results <- extract_results(switzerland_model, "Switzerland")
regional_results <- bind_rows(cleveland_results, hungarian_results,
                               long_beach_results, switzerland_results)

# View the consolidated results
regional_results
```


```{r}
male_data <- cleaned_data %>% filter(sex == 1)
female_data <- cleaned_data %>% filter(sex == 0)

male_model <- glm(num ~ age + cp + trestbps + chol + fbs +
                  restecg + thalach + exang + oldpeak,
                  data = male_data, family = binomial)
female_model <- glm(num ~ age + cp + trestbps + chol + fbs +
                    restecg + thalach + exang + oldpeak,
                    data = female_data, family = binomial)
# Function to extract model results
extract_results <- function(model, gender) {
  coefficients <- summary(model)$coefficients
  data.frame(
    Gender = gender,
    Variable = rownames(coefficients),
    Estimate = coefficients[, "Estimate"],
    Std_Error = coefficients[, "Std. Error"],
    P_Value = coefficients[, "Pr(>|z|)"]
  )
}

# Extract results for males and females
male_results <- extract_results(male_model, "Male")
female_results <- extract_results(female_model, "Female")
gender_results <- bind_rows(male_results, female_results)

```

Males
Significant Predictors (p-value < 0.05):

Chest Pain (cp) (β=0.624, p<0.001):
  Higher chest pain levels increase the odds of heart disease.
  Odds Ratio= exp(0.624)=1.87.
Max Heart Rate (thalach) (β=−0.026, p<0.001):
  Higher maximum heart rates reduce the odds of heart disease.
  Odds Ratio= exp(−0.026)=0.974 (protective effect).
Exercise-Induced Angina (exang) (β=0.717, p=0.025):
  Presence of exercise-induced angina significantly increases the odds of heart    disease.
  Odds Ratio=exp(0.717)=2.05.
ST Depression (oldpeak) (β=0.569, p<0.001):
  Higher ST depression significantly increases the odds of heart disease.
Odds Ratio = exp(0.569)=1.77.

Non-Significant Predictors:
age, trestbps, chol, fbs, restecg.

Females
Significant Predictors (p-value < 0.05):

Chest Pain (cp) (β=0.927, p=0.006):
  Stronger effect than males.
  Odds Ratio = exp(0.927)=2.53.
Exercise-Induced Angina (exang) (β=1.144, p=0.037):
  Stronger effect than males.
  Odds Ratio = exp(1.144)=3.14.
ST Depression (oldpeak) (β=0.677,p=0.020):
  Similar effect to males.
  Odds Ratio = exp(0.677)=1.97.
Non-Significant Predictors:
age, trestbps, chol, fbs, restecg, thalach.

Conclusions
Predictors for Both Genders:
cp, exang, and oldpeak are significant predictors for both males and females.

Gender-Specific Differences:
Stronger effects of cp and exang in females suggest potential gender-specific diagnostic markers for heart disease.



(author: Yonghao YU)

## Try Random Forest Classifier!

### A brief intro to Random Forest Algorithm

Random Forest is an ensemble learning algorithm used for classification and regression tasks. It builds multiple decision trees using bootstrap sampling (random subsets of data) and selects features randomly at each split to increase diversity.   Each tree predicts independently, and the final output is determined by majority voting (classification) or averaging (regression).   Random Forest is robust to overfitting, handles high-dimensional data well, and provides feature importance scores.


### First, construct the model and generate the results! 
author: Yonghao YU
```{r}
library(caret)
library(randomForest)

# drop out the variable "ca" and "thal" which are have so many missing values inside
variables = c("chol", "cp", "age", "thalach", "oldpeak", "num", "restecg", "fbs", "region", "slope", "trestbps", "exang")
data = combined_data_two[, variables]
data$num = as.factor(data$num)

# check and deal with missing data
if (any(is.na(data))) {
  print("Missing value detected")
  data = na.omit(data)
  print("Missing data have been deleted")
}
# split the dataset into training and testing datasets
set.seed(42)
trainIndex = createDataPartition(data$num, p = 0.8, list = FALSE)
trainData = data[trainIndex, ]
testData = data[-trainIndex, ]

# Construct the random forest model and evaluate the model results
rf_model = randomForest(num ~ ., data = trainData, importance = TRUE)
rf_pred = predict(rf_model, testData)
rf_conf_matrix = confusionMatrix(rf_pred, testData$num)
print("The model result is")
print(rf_conf_matrix)
```

From the model we can observe the following things: 
1. The model correctly classified 77.14% of the instances.
2. The true accuracy is expected to fall 95% of the time in (0.6793, 0.8477)
3. The information rate is 0.6095 which is less than the accuracy rate (p-value also indicate this), indicating the model we built actually capture some significant features.
4. The Kappa is 0.5281 which is in the range [40,60], which indicate our classifier achieves moderate level of classification
5. High sensitivity (0.7561) indicates good identification of positives.
6. High specificity (0.7812) indicates good identification of negatives.
7. Balanced accuracy (76.87%) suggests the model balances its performance across both classes well.


author: Yonghao YU
### Then investigate the AUC value and ROC curve to assess the model's ability

```{r}
library(pROC)
# Generate AUC value
rf_prob = predict(rf_model, testData, type = "prob")
roc_curve = roc(testData$num, rf_prob[, 2], levels = rev(levels(testData$num)))
auc_value = auc(roc_curve)
roc_data = data.frame(
  FPR = 1 - roc_curve$specificities,
  TPR = roc_curve$sensitivities
)

# Plot the ROC curve
ggplot(data = roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = "blue", size = 1) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(
    title = "ROC Curve for Random Forest",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    subtitle = paste("AUC =", round(auc_value, 2))
  ) +
  theme_minimal()
```

The AUC value of 0.83 indicates that the Random Forest model performs well in distinguishing between positive and negative classes. Specifically, there is an 83% chance that the model will rank a randomly chosen positive instance higher than a negative one. This reflects our model reaches good discrimination.

The ROC curve shows the trade-off between the True Positive Rate (Sensitivity) and the False Positive Rate (1 - Specificity) at various thresholds. The curve is well above the diagonal which represent random guessing, confirming the model performs better than random guessing. The initial steep rise indicates that the model achieves high sensitivity with a relatively low false positive rate, which is desirable. However, as the false positive rate increases, the curve flattens, highlighting diminishing returns in improving sensitivity further.


author: Yonghao YU
### Then we show the feature importance trends(The trend is descending according to the MeanDecreaseAccuracy)
```{r,fig.width=10, fig.height=8}
var_imp = importance(rf_model)
var_imp_df = as.data.frame(var_imp)
var_imp_df$Variable = rownames(var_imp_df)
rownames(var_imp_df) = NULL
var_imp_df = var_imp_df[order(var_imp_df$MeanDecreaseAccuracy, decreasing = TRUE), ]
ggplot(var_imp_df, aes(x = reorder(Variable, -MeanDecreaseAccuracy))) +
  geom_line(aes(y = MeanDecreaseAccuracy, group = 1, color = "MeanDecreaseAccuracy")) +
  geom_point(aes(y = MeanDecreaseAccuracy, color = "MeanDecreaseAccuracy")) +
  geom_line(aes(y = MeanDecreaseGini, group = 1, color = "MeanDecreaseGini")) +
  geom_point(aes(y = MeanDecreaseGini, color = "MeanDecreaseGini")) +
  labs(title = "Feature Importance Trends",
       x = "Features",
       y = "Importance",
       color = "Metric") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1))
```  


Then we ranked the predictors descendingly based on the MeanDecreaseAccuracy which measures the decrease in overall model accuracy when the variable is permuted. And we show them in a line plot! Based on the MeanDecreaseAccuracy and MeanDecreaseGini, we can drop out restecg, trestbps, and fbs predictors that have relatively small impact on our prediction. And then we can focus on the first eight predictors that have more impact on our prediction results!

### Last, simulate the prediction which predicts the num with the new data based on the model we built!

author: Yonghao YU
```{r}
# construct a new dataframe which includes new data
new_data = data.frame(
  age = c(63,39),
  sex = c(1, 1),
  cp = c(1, 2),
  trestbps = c(145, 120),
  chol = c(233, 200),
  fbs = c(1, 0),
  restecg = c(2, 0),
  thalach = c(150, 160),
  exang = c(0, 1),
  oldpeak = c(2.3, 1),
  slope = c(3, 2),
  region = c(1, 2)
)
# predict the results based on the model we have trained
predicted_num = predict(rf_model, new_data)
print("The prediction result is：")
print(data.frame(new_data, Predicted_num = predicted_num))
```

From the results, we can see that the model can generate some results based on the predictor values we put in!
