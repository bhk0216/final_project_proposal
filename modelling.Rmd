---
title: "Modelling"
author: "Yixin Zheng (yz4993), Thomas Tang (tt3022), Yonghao YU (yy3564)"
date: "2024-11-16"
output: github_document
---

## Yixin Zheng (yz4993), Thomas Tang (tt3022), Yonghao YU (yy3564)
```{r setup}
library(tidyverse)
library(janitor)
```

```{r import data author: Yixin Zheng, Stella Koo, include = FALSE, eval=FALSE}
save_path <- "./data"

column_names <- c("age", "sex", "cp", "trestbps", "chol", "fbs", 
                  "restecg", "thalach", "exang", "oldpeak", "slope", 
                  "ca", "thal", "num")

process_data <- function(file_path, save_name) {

    data <- read.table(file_path, sep = ",", header = FALSE)
    
    colnames(data) <- column_names
    
    write.csv(data, file.path(save_path, save_name), row.names = FALSE)
}

process_data("./heart+disease/processed.cleveland.data", "cleveland.csv")
process_data("./heart+disease/processed.hungarian.data", "hungarian.csv")
process_data("./heart+disease/processed.va.data", "long_beach_va.csv")
process_data("./heart+disease/processed.switzerland.data", "switzerland.csv")
```


Interpreting num (author: Yixin Zheng):
The values for `num` represent the degree of narrowing in the coronary arteries:
0: No disease (< 50% diameter narrowing).
1-4: Increasing severity of disease (> 50% diameter narrowing, with different severities).

For convenience, this variable will binarized:
0: No heart disease (value 0 in num).
1: Presence of heart disease (values 1-4 in num).

*but if we want to analyze the severity of heart disease num will be treated as a categorical variable.
example code:
cleaned_data <- data |> 
  mutate(num = factor(num, levels = c(0, 1, 2, 3, 4), 
                      labels = c("No Disease", "Mild", "Moderate", "Severe", "Very Severe")))
                      
```{r data cleaning author: Yixin Zheng}
cleveland <- read_csv("./data/cleveland.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1)) # Binarize the `num` variable: 0 = no heart disease, 1 = heart disease

# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

hungary = read_csv("./data/hungarian.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

long_beach = read_csv("./data/long_beach_va.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed)

switzerland = read_csv("./data/switzerland.csv", na = "?") |> 
  clean_names() |> 
  mutate(num = if_else(num == 0, 0, 1))
# |> drop_na() Removes rows with any missing values (optional, adjust as needed) 

```

```{r}
cor(cleveland$chol, cleveland$num, use = "complete.obs")
```


hypothesis (author: Yixin Zheng)
* need to run some test choose explanatory variables?

1. Comparing Diagnostic Factors for Heart Disease Across Regions
 explore whether certain diagnostic factors (e.g., cholesterol levels, exercise-induced angina) are more predictive of heart disease in one region compared to others.
 
* there are many null values in the `chol` column in switzerland dataset. So maybe we need to use other diagnostic factors such as `trestbps` if we want to compare the regions
Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between cholesterol levels (`chol`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a positive linear relationship between cholesterol levels (`chol`) and the presence of heart disease (`num`). ($\beta_1>0$)
  Where:
  - `num` is the target variable indicating heart disease presence.
  - `chol` is the predictor variable representing cholesterol levels.

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \epsilon $$

Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Cholesterol levels (`chol`), blood pressure (`trestbps`), and exercise-induced angina (`exang`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`chol`, `trestbps`, `exang`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \beta_2 \cdot \text{trestbps} + \beta_3 \cdot \text{exang} + \epsilon $$

2. Examining Predictive Power of Clinical Indicators for Heart Disease in Diverse Populations

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between maximum heart rate achieved (`thalach`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a negative linear relationship between maximum heart rate achieved (`thalach`) and the presence of heart disease (`num`). ($\beta_1<0$)

$$ num = \beta_0 + \beta_1 \cdot \text{thalach} + \epsilon $$
Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Maximum heart rate (`thalach`), fasting blood sugar (`fbs`), and the slope of the ST segment (`slope`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`thalach`, `fbs`, `slope`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{thalach} + \beta_2 \cdot \text{fbs} + \beta_3 \cdot \text{slope} + \epsilon $$

3. Influence of Age and Lifestyle Factors on Heart Disease

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between age (`age`) and the presence of heart disease (`num`).
- **Alternative Hypothesis (H_a):** There is a positive linear relationship between age (`age`) and the presence of heart disease (`num`). ($\beta_1>0$)

$$ num = \beta_0 + \beta_1 \cdot \text{age} + \epsilon $$
Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Age (`age`), chest pain type (`cp`), and exercise-induced angina (`exang`) are not significant predictors of heart disease presence (`num`).
- **Alternative Hypothesis (H_a):** At least one of these variables (`age`, `cp`, `exang`) is a significant predictor of heart disease presence (`num`).

$$ num = \beta_0 + \beta_1 \cdot \text{age} + \beta_2 \cdot \text{cp} + \beta_3 \cdot \text{exang} + \epsilon $$

4. Regional Patterns in Heart Disease Diagnostic Attributes

Example Hypothesis (SLR):
- **Null Hypothesis (H_0):** There is no linear relationship between diagnostic attributes (e.g., cholesterol, age) and heart disease (`num`) in each region.
- **Alternative Hypothesis (H_a):**The relationship between diagnostic attributes (e.g., cholesterol, age) and heart disease (`num`) differs significantly across regions.
- This hypothesis can be tested by performing SLR for each region and comparing the coefficients (beta_1 values) to see if they vary.

Example Hypothesis (MLR):
- **Null Hypothesis (H_0):** Regional differences do not significantly influence the predictive power of diagnostic factors for heart disease.
- **Alternative Hypothesis (H_a):** Regional differences significantly influence the predictive power of diagnostic factors for heart disease.

$$ num = \beta_0 + \beta_1 \cdot \text{chol} + \beta_2 \cdot \text{age} + \beta_3 \cdot \text{region} + \beta_4 \cdot (\text{chol} \times \text{region}) + \beta_5 \cdot (\text{age} \times \text{region}) + \epsilon $$


author: Thomas Tang
```{r }
cleveland <- read.csv("./data/cleveland.csv", header = FALSE)
hungarian <- read.csv("./data/hungarian.csv", header = FALSE)
long_beach <- read.csv("./data/long_beach_va.csv", header = FALSE)
switzerland <- read.csv("./data/switzerland.csv", header = FALSE)

# Add region column and combine datasets
cleveland$region <- "Cleveland"
hungarian$region <- "Hungarian"
long_beach$region <- "Long_Beach_VA"
switzerland$region <- "Switzerland"
combined_data <- bind_rows(cleveland, hungarian, long_beach, switzerland)

colnames(combined_data) <- c("age", "sex", "cp", "trestbps", "chol", "fbs",
                             "restecg", "thalach", "exang", "oldpeak", "slope",
                             "ca", "thal", "num", "region")
combined_data <- combined_data %>%
  mutate(across(c(age, sex, cp, trestbps, chol, fbs, restecg, thalach,
                  exang, oldpeak, slope, ca, thal, num), as.numeric))
```
Dropped variables with excessive missing values (ca and thal).
Removed rows with missing values in critical variables.
Converted num to binary (0 = no heart disease, 1 = heart disease) and set as a factor.
```{R}
cleaned_data <- combined_data %>% select(-ca, -thal)
critical_columns <- c("num", "age", "sex", "cp", "trestbps", "chol",
                      "fbs", "restecg", "thalach", "exang", "oldpeak", "slope")
cleaned_data <- cleaned_data %>% drop_na(all_of(critical_columns))
summary(cleaned_data)
```
```{r author: Thomas Tang}
cleaned_data$num <- ifelse(cleaned_data$num > 0, 1, 0)
cleaned_data$num <- as.factor(cleaned_data$num)
logistic_model <- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                      restecg + thalach + exang + oldpeak,
                      data = cleaned_data, family = binomial)
summary(logistic_model)
```
(author= Thomas Tang)
Significant Predictors (p-value < 0.05):
sex (1.409350):
Males are significantly more likely to have heart disease than females.
Odds ratio: exp(1.409)=4.09 → Males are 4.09 times more likely to have heart disease compared to females.
cp (0.687260):
Higher chest pain levels increase the odds of heart disease.
Odds ratio: exp(0.687)=1.99.
trestbps (0.013883):
Resting blood pressure has a small but significant positive effect.
Odds ratio: exp(0.0139)=1.014 per unit increase.
chol (-0.003090):
Cholesterol has a small negative association.
Odds ratio: exp(−0.0031)=0.997, slightly decreasing odds.
thalach (-0.021518):
Higher maximum heart rate achieved decreases the odds of heart disease.
Odds ratio: exp(−0.0215)=0.978 → Protective effect.
exang (0.894064):
Exercise-induced angina increases the odds of heart disease. exp(0.894)=2.44.
oldpeak (0.586433):Higher ST depression values significantly increase the odds of heart disease.
Odds ratio: exp(0.586)=1.80.
fbs (Fasting blood sugar) and age do not have significant association with heart disease in this dataset.

AIC: 469.38 (used for model comparison).



(author: Yonghao YU)

## Try Random Forest Classifier!

### First, do data preprocessing! 

```{r, author: Yonghao YU}
cleveland$region = "Cleveland"
hungary$region = "Hungarian"
long_beach$region = "Long_Beach_VA"
switzerland$region = "Switzerland"
combined_data = bind_rows(cleveland, hungary, long_beach, switzerland)

colnames(combined_data) = c("age", "sex", "cp", "trestbps", "chol", "fbs",
                             "restecg", "thalach", "exang", "oldpeak", "slope",
                             "ca", "thal", "num", "region")
combined_data = combined_data |>
  mutate(across(c(age, sex, cp, trestbps, chol, fbs, restecg, thalach,
                  exang, oldpeak, slope, ca, thal, num), as.numeric)) |>
  mutate(region = case_when(
    region == "Cleveland" ~ 1,
    region == "Hungarian" ~ 2,
    region == "Long_Beach_VA" ~ 3,
    region == "Switzerland" ~ 4,
  ))
print(combined_data)
```

### Second, construct the model and generate the results! 

```{r, author: Yonghao YU}
library(caret)
library(randomForest)

# drop out the variable "ca" and "thal" which are have so many missing values inside
variables = c("chol", "cp", "age", "thalach", "oldpeak", "num", "restecg", "fbs", "region", "slope", "trestbps", "exang")
data = combined_data[, variables]
data$num = as.factor(data$num)

# check and deal with missing data
if (any(is.na(data))) {
  print("Missing value detected")
  data = na.omit(data)
  print("Missing data have been deleted")
}
# split the dataset into training and testing datasets
set.seed(42)
trainIndex = createDataPartition(data$num, p = 0.8, list = FALSE)
trainData = data[trainIndex, ]
testData = data[-trainIndex, ]

# Construct the random forest model and evaluate the model results
rf_model = randomForest(num ~ ., data = trainData, importance = TRUE)
rf_pred = predict(rf_model, testData)
rf_conf_matrix = confusionMatrix(rf_pred, testData$num)
print("The model result is：")
print(rf_conf_matrix)

# Check out the importance of the predictors and sort them descendingly based on MeanDecreaseGini
var_imp = importance(rf_model)
sorted_var_imp = var_imp[order(var_imp[, "MeanDecreaseAccuracy"], decreasing = TRUE), ]
print("Sort based on MeanDecreaseGini：")
print(sorted_var_imp)
```

From the model we can observe the following things: 
1. The model correctly classified 77.14% of the instances.
2. The true accuracy is expected to fall 95% of the time in (0.6793, 0.8477)
3. The information rate is 0.6095 which is less than the accuracy rate (p-value also indicate this), indicating the model we built actually capture some significant features.
4. The Kappa is 0.5281 which is in the range [40,60], which indicate our classifier achieves moderate level of classification
5. High sensitivity (0.7561) indicates good identification of positives.
6. High specificity (0.7812) indicates good identification of negatives.
7. Balanced accuracy (76.87%) suggests the model balances its performance across both classes well.

Then we ranked the predictors descendingly based on the MeanDecreaseAccuracy which measures the decrease in overall model accuracy when the variable is permuted. Based on the MeanDecreaseAccuracy and MeanDecreaseGini, we can drop out restecg, trestbps, and fbs predictors that have relatively small impact on our prediction. And then we can focus on the first eight predictors that have more impact on our prediction results!

### Last, simulate the prediction which predicts the num with the new data based on the model we built!

```{r, author: Yonghao YU}
# construct a new dataframe which includes new data
new_data = data.frame(
  age = c(63,39),
  sex = c(1, 1),
  cp = c(1, 2),
  trestbps = c(145, 120),
  chol = c(233, 200),
  fbs = c(1, 0),
  restecg = c(2, 0),
  thalach = c(150, 160),
  exang = c(0, 1),
  oldpeak = c(2.3, 1),
  slope = c(3, 2),
  region = c(1, 2)
)
# predict the results based on the model we have trained
predicted_num = predict(rf_model, new_data)
print("The prediction result is：")
print(data.frame(new_data, Predicted_num = predicted_num))
```

From the results, we can see that the model can generate some results based on the predictor values we put in!
