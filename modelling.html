<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>modelling</title>

<script src="site_libs/header-attrs-2.28/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="proposal.html">About</a>
</li>
<li>
  <a href="final_report.html">Report</a>
</li>
<li>
  <a href="EDA.html">EDA</a>
</li>
<li>
  <a href="Modelling.html">Modelling</a>
</li>
<li>
  <a href="dashboard.html">Dashboard</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">




</div>


<div id="modelling" class="section level1">
<h1>Modelling</h1>
<p>Yixin Zheng (yz4993), Thomas Tang (tt3022), Yonghao YU (yy3564)
2024-11-16</p>
<div id="yixin-zheng-yz4993-thomas-tang-tt3022-yonghao-yu-yy3564"
class="section level2">
<h2>Yixin Zheng (yz4993), Thomas Tang (tt3022), Yonghao YU (yy3564)</h2>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.4     ✔ readr     2.1.5
## ✔ forcats   1.0.0     ✔ stringr   1.5.1
## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
## ✔ purrr     1.0.2     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
<pre class="r"><code>library(janitor)</code></pre>
<pre><code>## 
## 载入程序包：&#39;janitor&#39;
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     chisq.test, fisher.test</code></pre>
<p>Interpreting num (author: Yixin Zheng): The values for
<code>num</code> represent the degree of narrowing in the coronary
arteries: 0: No disease (&lt; 50% diameter narrowing). 1-4: Increasing
severity of disease (&gt; 50% diameter narrowing, with different
severities).</p>
<p>For convenience, this variable will binarized: 0: No heart disease
(value 0 in num). 1: Presence of heart disease (values 1-4 in num).</p>
<p>*but if we want to analyze the severity of heart disease num will be
treated as a categorical variable. example code: cleaned_data &lt;- data
|&gt; mutate(num = factor(num, levels = c(0, 1, 2, 3, 4), labels = c(“No
Disease”, “Mild”, “Moderate”, “Severe”, “Very Severe”)))</p>
<pre class="r"><code>cleveland &lt;- read_csv(&quot;./data/cleveland.csv&quot;, na = &quot;?&quot;) |&gt; 
  clean_names() |&gt; 
  mutate(num = if_else(num == 0, 0, 1)) # Binarize the `num` variable: 0 = no heart disease, 1 = heart disease</code></pre>
<pre><code>## Rows: 303 Columns: 14
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># |&gt; drop_na() Removes rows with any missing values (optional, adjust as needed)

hungary = read_csv(&quot;./data/hungarian.csv&quot;, na = &quot;?&quot;) |&gt; 
  clean_names() |&gt; 
  mutate(num = if_else(num == 0, 0, 1))</code></pre>
<pre><code>## Rows: 294 Columns: 14
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># |&gt; drop_na() Removes rows with any missing values (optional, adjust as needed)

long_beach = read_csv(&quot;./data/long_beach_va.csv&quot;, na = &quot;?&quot;) |&gt; 
  clean_names() |&gt; 
  mutate(num = if_else(num == 0, 0, 1))</code></pre>
<pre><code>## Rows: 200 Columns: 14
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># |&gt; drop_na() Removes rows with any missing values (optional, adjust as needed)

switzerland = read_csv(&quot;./data/switzerland.csv&quot;, na = &quot;?&quot;) |&gt; 
  clean_names() |&gt; 
  mutate(num = if_else(num == 0, 0, 1))</code></pre>
<pre><code>## Rows: 123 Columns: 14
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># |&gt; drop_na() Removes rows with any missing values (optional, adjust as needed) </code></pre>
<pre class="r"><code>cor(cleveland$chol, cleveland$num, use = &quot;complete.obs&quot;)</code></pre>
<pre><code>## [1] 0.08516361</code></pre>
</div>
</div>
<div id="variable-selection-author-yonghao-yu" class="section level1">
<h1>Variable Selection (author: Yonghao YU)</h1>
<p>author: Yonghao YU</p>
<div id="data-preprocessing" class="section level3">
<h3>Data Preprocessing</h3>
<pre class="r"><code>cleveland$region = &quot;Cleveland&quot;
hungary$region = &quot;Hungarian&quot;
long_beach$region = &quot;Long_Beach_VA&quot;
switzerland$region = &quot;Switzerland&quot;
combined_data_one = bind_rows(cleveland, hungary, long_beach, switzerland)

colnames(combined_data_one) = c(&quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;fbs&quot;,
                             &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, &quot;slope&quot;,
                             &quot;ca&quot;, &quot;thal&quot;, &quot;num&quot;, &quot;region&quot;)
combined_data_two = combined_data_one |&gt;
  mutate(region = case_when(
    region == &quot;Cleveland&quot; ~ 1,
    region == &quot;Hungarian&quot; ~ 2,
    region == &quot;Long_Beach_VA&quot; ~ 3,
    region == &quot;Switzerland&quot; ~ 4,
  )) |&gt;
  select(-thal,-ca) |&gt;
  drop_na()
case_data = combined_data_two |&gt;
  filter(num == 1)
control_data = combined_data_two |&gt;
  filter(num == 0)
print(case_data)</code></pre>
<pre><code>## # A tibble: 324 × 13
##      age   sex    cp trestbps  chol   fbs restecg thalach exang oldpeak slope
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1    67     1     4      160   286     0       2     108     1     1.5     2
##  2    67     1     4      120   229     0       2     129     1     2.6     2
##  3    62     0     4      140   268     0       2     160     0     3.6     3
##  4    63     1     4      130   254     0       2     147     0     1.4     2
##  5    53     1     4      140   203     1       2     155     1     3.1     3
##  6    56     1     3      130   256     1       2     142     1     0.6     2
##  7    48     1     2      110   229     0       0     168     0     1       3
##  8    58     1     2      120   284     0       2     160     0     1.8     2
##  9    58     1     3      132   224     0       2     173     0     3.2     1
## 10    60     1     4      130   206     0       2     132     1     2.4     2
## # ℹ 314 more rows
## # ℹ 2 more variables: num &lt;dbl&gt;, region &lt;dbl&gt;</code></pre>
<pre class="r"><code>print(control_data)</code></pre>
<pre><code>## # A tibble: 207 × 13
##      age   sex    cp trestbps  chol   fbs restecg thalach exang oldpeak slope
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1    63     1     1      145   233     1       2     150     0     2.3     3
##  2    37     1     3      130   250     0       0     187     0     3.5     3
##  3    41     0     2      130   204     0       2     172     0     1.4     1
##  4    56     1     2      120   236     0       0     178     0     0.8     1
##  5    57     0     4      120   354     0       0     163     1     0.6     1
##  6    57     1     4      140   192     0       0     148     0     0.4     2
##  7    56     0     2      140   294     0       2     153     0     1.3     2
##  8    44     1     2      120   263     0       0     173     0     0       1
##  9    52     1     3      172   199     1       0     162     0     0.5     1
## 10    57     1     3      150   168     0       0     174     0     1.6     1
## # ℹ 197 more rows
## # ℹ 2 more variables: num &lt;dbl&gt;, region &lt;dbl&gt;</code></pre>
<pre class="r"><code>print(combined_data_two)</code></pre>
<pre><code>## # A tibble: 531 × 13
##      age   sex    cp trestbps  chol   fbs restecg thalach exang oldpeak slope
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1    63     1     1      145   233     1       2     150     0     2.3     3
##  2    67     1     4      160   286     0       2     108     1     1.5     2
##  3    67     1     4      120   229     0       2     129     1     2.6     2
##  4    37     1     3      130   250     0       0     187     0     3.5     3
##  5    41     0     2      130   204     0       2     172     0     1.4     1
##  6    56     1     2      120   236     0       0     178     0     0.8     1
##  7    62     0     4      140   268     0       2     160     0     3.6     3
##  8    57     0     4      120   354     0       0     163     1     0.6     1
##  9    63     1     4      130   254     0       2     147     0     1.4     2
## 10    53     1     4      140   203     1       2     155     1     3.1     3
## # ℹ 521 more rows
## # ℹ 2 more variables: num &lt;dbl&gt;, region &lt;dbl&gt;</code></pre>
<p>author: Yonghao YU</p>
</div>
<div id="for-continues-case" class="section level3">
<h3>For Continues case</h3>
<p>For continuous variables, we use mean and standard deviation (std) to
describe the distribution in overall samples, samples of control(num =
0), and samples of case(num = 1). Then, we use t-test to examine whether
the means of these variables are significantly different between case
group and control group (significance level = 0.05).</p>
<pre class="r"><code># 1. Mean and Std for Continuous Variables (Overall)
list_conti_all = list(
  age = combined_data_two$age,
  trestbps = combined_data_two$trestbps,
  chol = combined_data_two$chol,
  thalach = combined_data_two$thalach,
  oldpeak = combined_data_two$oldpeak
) |&gt; 
  lapply(na.omit) 

mean_all = sapply(list_conti_all, mean) |&gt; 
  as.data.frame()|&gt;
  setNames(&quot;Overall Mean&quot;)

std_all = sapply(list_conti_all, sd) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Overall Std&quot;)

# 2. p-value of t-test for Continuous Variables
t_test = function(variable) {
  t_test_result = t.test(combined_data_two[[variable]] ~ combined_data_two$num)
  return(data.frame(
    variable = variable,
    p_value = t_test_result$p.value
  ))
}

p_value = 
  lapply(c(&quot;age&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;), t_test) |&gt; 
  bind_rows() |&gt; 
  as.data.frame()

# 3. Mean and Std for Control Group
list_conti_control = list(
  age = control_data$age,
  trestbps = control_data$trestbps,
  chol = control_data$chol,
  thalach = control_data$thalach,
  oldpeak = control_data$oldpeak
) |&gt; 
  lapply(na.omit)

mean_control = sapply(list_conti_control, mean) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Control Mean&quot;)

std_control = sapply(list_conti_control, sd) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Control Std&quot;)

# 4. Mean and Std for Case Group
list_conti_case = list(
  age = case_data$age,
  trestbps = case_data$trestbps,
  chol = case_data$chol,
  thalach = case_data$thalach,
  oldpeak = case_data$oldpeak
) |&gt; 
  lapply(na.omit)

mean_case = sapply(list_conti_case, mean) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Case Mean&quot;)

std_case = sapply(list_conti_case, sd) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Case Std&quot;)

conti_des_df =
  as.data.frame(cbind(mean_all, std_all, mean_control, std_control, mean_case, std_case, p_value))
conti_des_df = conti_des_df[, -grep(&quot;variable&quot;, colnames(conti_des_df))] |&gt; 
  knitr::kable(digits = 6)
conti_des_df</code></pre>
<table>
<colgroup>
<col width="10%" />
<col width="14%" />
<col width="13%" />
<col width="14%" />
<col width="13%" />
<col width="12%" />
<col width="12%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Overall Mean</th>
<th align="right">Overall Std</th>
<th align="right">Control Mean</th>
<th align="right">Control Std</th>
<th align="right">Case Mean</th>
<th align="right">Case Std</th>
<th align="right">p_value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">age</td>
<td align="right">54.843691</td>
<td align="right">8.824069</td>
<td align="right">52.908213</td>
<td align="right">9.248788</td>
<td align="right">56.080247</td>
<td align="right">8.323177</td>
<td align="right">0.000074</td>
</tr>
<tr class="even">
<td align="left">trestbps</td>
<td align="right">133.406780</td>
<td align="right">18.969496</td>
<td align="right">129.734300</td>
<td align="right">16.322060</td>
<td align="right">135.753086</td>
<td align="right">20.158831</td>
<td align="right">0.000179</td>
</tr>
<tr class="odd">
<td align="left">chol</td>
<td align="right">216.854991</td>
<td align="right">99.014215</td>
<td align="right">237.043478</td>
<td align="right">68.313903</td>
<td align="right">203.956790</td>
<td align="right">112.615863</td>
<td align="right">0.000030</td>
</tr>
<tr class="even">
<td align="left">thalach</td>
<td align="right">138.463277</td>
<td align="right">25.833649</td>
<td align="right">152.758454</td>
<td align="right">22.958375</td>
<td align="right">129.330247</td>
<td align="right">23.329890</td>
<td align="right">0.000000</td>
</tr>
<tr class="odd">
<td align="left">oldpeak</td>
<td align="right">1.218456</td>
<td align="right">1.105150</td>
<td align="right">0.726087</td>
<td align="right">0.805741</td>
<td align="right">1.533025</td>
<td align="right">1.155598</td>
<td align="right">0.000000</td>
</tr>
</tbody>
</table>
<p>Based on the result, we can find that all five features are
significantly different between case and control.</p>
<p>author: Yonghao YU</p>
</div>
<div id="for-discrete-case" class="section level3">
<h3>For Discrete case</h3>
<p>For binary and categorical variables, we use count (n) and percentage
(pct) to describe the distribution in overall samples, samples of
control(num = 0), and samples of case(num = 1). Then, as the data meet
the assumption, we use chi-sq test to examine whether the distribution
of these variables are significantly different between case group and
control group (significance level = 0.05).</p>
<pre class="r"><code>list_cat_all = as.data.frame(list(
  sex = combined_data_two$sex,
  cp = combined_data_two$cp,
  fbs = combined_data_two$fbs,
  restecg = combined_data_two$restecg,
  exang = combined_data_two$exang,
  slope = combined_data_two$slope,
  region = combined_data_two$region
))

# 1. Overall Counts and Chi-Square Test
cat_vars = names(list_cat_all)

count_all_function = function(variable) {
  table_value = table(list_cat_all[[variable]], combined_data_two$num) 
  chi_sq_test = chisq.test(table_value)
  
  count = table(list_cat_all[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    n = as.numeric(count),
    pct = round(pct, 3),
    p_value = round(chi_sq_test$p.value, 3)
  )
  
  return(result_df)
}

cat_count_chisq = lapply(cat_vars, count_all_function) |&gt; 
  bind_rows()

# 2. Control Group Counts and Percentages
list_cat_ctrl = as.data.frame(list(
  sex = control_data$sex,
  cp = control_data$cp,
  fbs = control_data$fbs,
  restecg = control_data$restecg,
  exang = control_data$exang,
  slope = control_data$slope,
  region = control_data$region
))

cat_vars_ctrl = names(list_cat_ctrl)

count_ctrl_function = function(variable) {
  count = table(list_cat_ctrl[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    control_n = as.numeric(count),
    control_pct = round(pct, 3)
  )
  
  return(result_df)
}

cat_count_ctrl = lapply(cat_vars_ctrl, count_ctrl_function) |&gt; 
  bind_rows()

# 3. Case Group Counts and Percentages
list_cat_case = as.data.frame(list(
  sex = case_data$sex,
  cp = case_data$cp,
  fbs = case_data$fbs,
  restecg = case_data$restecg,
  exang = case_data$exang,
  slope = case_data$slope,
  region = case_data$region
))

cat_vars_case = names(list_cat_case)

count_case_function = function(variable) {
  count = table(list_cat_case[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    case_n = as.numeric(count),
    case_pct = round(pct, 3)
  )
  return(result_df)
}

cat_count_case = lapply(cat_vars_case, count_case_function) |&gt; 
  bind_rows()

# 4. Combine Results
final_cat_count = cat_count_chisq |&gt;
  left_join(cat_count_ctrl, by = c(&quot;variable&quot;, &quot;category&quot;)) |&gt;
  left_join(cat_count_case, by = c(&quot;variable&quot;, &quot;category&quot;))|&gt;
  knitr::kable(digits = 3)

print(final_cat_count)</code></pre>
<pre><code>## 
## 
## |variable |category |   n|   pct| p_value| control_n| control_pct| case_n| case_pct|
## |:--------|:--------|---:|-----:|-------:|---------:|-----------:|------:|--------:|
## |sex      |0        | 127| 0.239|   0.000|        87|       0.420|     40|    0.123|
## |sex      |1        | 404| 0.761|   0.000|       120|       0.580|    284|    0.877|
## |cp       |1        |  30| 0.056|   0.000|        18|       0.087|     12|    0.037|
## |cp       |2        |  70| 0.132|   0.000|        51|       0.246|     19|    0.059|
## |cp       |3        | 114| 0.215|   0.000|        80|       0.386|     34|    0.105|
## |cp       |4        | 317| 0.597|   0.000|        58|       0.280|    259|    0.799|
## |fbs      |0        | 446| 0.840|   0.378|       178|       0.860|    268|    0.827|
## |fbs      |1        |  85| 0.160|   0.378|        29|       0.140|     56|    0.173|
## |restecg  |0        | 297| 0.559|   0.000|       123|       0.594|    174|    0.537|
## |restecg  |1        |  73| 0.137|   0.000|        13|       0.063|     60|    0.185|
## |restecg  |2        | 161| 0.303|   0.000|        71|       0.343|     90|    0.278|
## |exang    |0        | 267| 0.503|   0.000|       163|       0.787|    104|    0.321|
## |exang    |1        | 264| 0.497|   0.000|        44|       0.213|    220|    0.679|
## |slope    |1        | 173| 0.326|   0.000|       119|       0.575|     54|    0.167|
## |slope    |2        | 310| 0.584|   0.000|        76|       0.367|    234|    0.722|
## |slope    |3        |  48| 0.090|   0.000|        12|       0.058|     36|    0.111|
## |region   |1        | 303| 0.571|   0.000|       164|       0.792|    139|    0.429|
## |region   |2        |  95| 0.179|   0.000|        27|       0.130|     68|    0.210|
## |region   |3        |  87| 0.164|   0.000|        15|       0.072|     72|    0.222|
## |region   |4        |  46| 0.087|   0.000|         1|       0.005|     45|    0.139|</code></pre>
<p>Based on the result, we can find that except fbs, the rest of all
other binary and categorical features are significantly different
between case and control.</p>
<p>hypothesis (author: Yixin Zheng) * need to run some test choose
explanatory variables?</p>
<ol style="list-style-type: decimal">
<li>Comparing Diagnostic Factors for Heart Disease Across Regions
explore whether certain diagnostic factors (e.g., cholesterol levels,
exercise-induced angina) are more predictive of heart disease in one
region compared to others.</li>
</ol>
<ul>
<li>there are many null values in the <code>chol</code> column in
switzerland dataset. So maybe we need to use other diagnostic factors
such as <code>trestbps</code> if we want to compare the regions Example
Hypothesis (SLR):</li>
<li><strong>Null Hypothesis (H_0):</strong> There is no linear
relationship between cholesterol levels (<code>chol</code>) and the
presence of heart disease (<code>num</code>).</li>
<li><strong>Alternative Hypothesis (H_a):</strong> There is a positive
linear relationship between cholesterol levels (<code>chol</code>) and
the presence of heart disease (<code>num</code>). (<span
class="math inline">\(\beta_1&gt;0\)</span>) Where:
<ul>
<li><code>num</code> is the target variable indicating heart disease
presence.</li>
<li><code>chol</code> is the predictor variable representing cholesterol
levels.</li>
</ul></li>
</ul>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{chol} + \epsilon \]</span></p>
<p>Example Hypothesis (MLR): - <strong>Null Hypothesis (H_0):</strong>
Cholesterol levels (<code>chol</code>), blood pressure
(<code>trestbps</code>), and exercise-induced angina
(<code>exang</code>) are not significant predictors of heart disease
presence (<code>num</code>). - <strong>Alternative Hypothesis
(H_a):</strong> At least one of these variables (<code>chol</code>,
<code>trestbps</code>, <code>exang</code>) is a significant predictor of
heart disease presence (<code>num</code>).</p>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{chol} + \beta_2 \cdot \text{trestbps} + \beta_3 \cdot \text{exang}
+ \epsilon \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Examining Predictive Power of Clinical Indicators for Heart Disease
in Diverse Populations</li>
</ol>
<p>Example Hypothesis (SLR): - <strong>Null Hypothesis (H_0):</strong>
There is no linear relationship between maximum heart rate achieved
(<code>thalach</code>) and the presence of heart disease
(<code>num</code>). - <strong>Alternative Hypothesis (H_a):</strong>
There is a negative linear relationship between maximum heart rate
achieved (<code>thalach</code>) and the presence of heart disease
(<code>num</code>). (<span
class="math inline">\(\beta_1&lt;0\)</span>)</p>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{thalach} + \epsilon \]</span> Example Hypothesis (MLR): -
<strong>Null Hypothesis (H_0):</strong> Maximum heart rate
(<code>thalach</code>), fasting blood sugar (<code>fbs</code>), and the
slope of the ST segment (<code>slope</code>) are not significant
predictors of heart disease presence (<code>num</code>). -
<strong>Alternative Hypothesis (H_a):</strong> At least one of these
variables (<code>thalach</code>, <code>fbs</code>, <code>slope</code>)
is a significant predictor of heart disease presence
(<code>num</code>).</p>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{thalach} + \beta_2 \cdot \text{fbs} + \beta_3 \cdot \text{slope} +
\epsilon \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Influence of Age and Lifestyle Factors on Heart Disease</li>
</ol>
<p>Example Hypothesis (SLR): - <strong>Null Hypothesis (H_0):</strong>
There is no linear relationship between age (<code>age</code>) and the
presence of heart disease (<code>num</code>). - <strong>Alternative
Hypothesis (H_a):</strong> There is a positive linear relationship
between age (<code>age</code>) and the presence of heart disease
(<code>num</code>). (<span
class="math inline">\(\beta_1&gt;0\)</span>)</p>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{age} + \epsilon \]</span> Example Hypothesis (MLR): - <strong>Null
Hypothesis (H_0):</strong> Age (<code>age</code>), chest pain type
(<code>cp</code>), and exercise-induced angina (<code>exang</code>) are
not significant predictors of heart disease presence (<code>num</code>).
- <strong>Alternative Hypothesis (H_a):</strong> At least one of these
variables (<code>age</code>, <code>cp</code>, <code>exang</code>) is a
significant predictor of heart disease presence (<code>num</code>).</p>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{age} + \beta_2 \cdot \text{cp} + \beta_3 \cdot \text{exang} +
\epsilon \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Regional Patterns in Heart Disease Diagnostic Attributes</li>
</ol>
<p>Example Hypothesis (SLR): - <strong>Null Hypothesis (H_0):</strong>
There is no linear relationship between diagnostic attributes (e.g.,
cholesterol, age) and heart disease (<code>num</code>) in each region. -
<strong>Alternative Hypothesis (H_a):</strong>The relationship between
diagnostic attributes (e.g., cholesterol, age) and heart disease
(<code>num</code>) differs significantly across regions. - This
hypothesis can be tested by performing SLR for each region and comparing
the coefficients (beta_1 values) to see if they vary.</p>
<p>Example Hypothesis (MLR): - <strong>Null Hypothesis (H_0):</strong>
Regional differences do not significantly influence the predictive power
of diagnostic factors for heart disease. - <strong>Alternative
Hypothesis (H_a):</strong> Regional differences significantly influence
the predictive power of diagnostic factors for heart disease.</p>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{chol} + \beta_2 \cdot \text{age} + \beta_3 \cdot \text{region} +
\beta_4 \cdot (\text{chol} \times \text{region}) + \beta_5 \cdot
(\text{age} \times \text{region}) + \epsilon \]</span></p>
<p>author: Thomas Tang</p>
<pre class="r"><code>cleveland &lt;- read.csv(&quot;./data/cleveland.csv&quot;, header = FALSE)
hungarian &lt;- read.csv(&quot;./data/hungarian.csv&quot;, header = FALSE)
long_beach &lt;- read.csv(&quot;./data/long_beach_va.csv&quot;, header = FALSE)
switzerland &lt;- read.csv(&quot;./data/switzerland.csv&quot;, header = FALSE)

# Add region column and combine datasets
cleveland$region &lt;- &quot;Cleveland&quot;
hungarian$region &lt;- &quot;Hungarian&quot;
long_beach$region &lt;- &quot;Long_Beach_VA&quot;
switzerland$region &lt;- &quot;Switzerland&quot;
combined_data &lt;- bind_rows(cleveland, hungarian, long_beach, switzerland)

colnames(combined_data) &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;fbs&quot;,
                             &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, &quot;slope&quot;,
                             &quot;ca&quot;, &quot;thal&quot;, &quot;num&quot;, &quot;region&quot;)
combined_data &lt;- combined_data %&gt;%
  mutate(across(c(age, sex, cp, trestbps, chol, fbs, restecg, thalach,
                  exang, oldpeak, slope, ca, thal, num), as.numeric))</code></pre>
<pre><code>## Warning: There were 14 warnings in `mutate()`.
## The first warning was:
## ℹ In argument: `across(...)`.
## Caused by warning:
## ! 强制改变过程中产生了NA
## ℹ Run `dplyr::last_dplyr_warnings()` to see the 13 remaining warnings.</code></pre>
<p>Dropped variables with excessive missing values (ca and thal).
Removed rows with missing values in critical variables. Converted num to
binary (0 = no heart disease, 1 = heart disease) and set as a
factor.</p>
<pre class="r"><code>cleaned_data &lt;- combined_data %&gt;% select(-ca, -thal)
critical_columns &lt;- c(&quot;num&quot;, &quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;,
                      &quot;fbs&quot;, &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, 
                      &quot;slope&quot;,&quot;region&quot;)
cleaned_data &lt;- cleaned_data %&gt;% drop_na(all_of(critical_columns))
cleaned_data$num &lt;- ifelse(cleaned_data$num &gt; 0, 1, 0)
cleaned_data$num &lt;- as.factor(cleaned_data$num)
logistic_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                      restecg + thalach + exang + oldpeak + region,
                      data = cleaned_data, family = binomial)
summary(logistic_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = num ~ age + sex + cp + trestbps + chol + fbs + 
##     restecg + thalach + exang + oldpeak + region, family = binomial, 
##     data = cleaned_data)
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         -4.529743   1.806627  -2.507 0.012166 *  
## age                  0.016202   0.016132   1.004 0.315231    
## sex                  1.469384   0.292048   5.031 4.87e-07 ***
## cp                   0.668141   0.138938   4.809 1.52e-06 ***
## trestbps             0.009865   0.007107   1.388 0.165114    
## chol                 0.001104   0.001820   0.607 0.544104    
## fbs                  0.089712   0.341956   0.262 0.793052    
## restecg              0.166720   0.141642   1.177 0.239175    
## thalach             -0.016074   0.006369  -2.524 0.011615 *  
## exang                0.899701   0.279483   3.219 0.001286 ** 
## oldpeak              0.696194   0.136346   5.106 3.29e-07 ***
## regionHungarian      0.179038   0.387015   0.463 0.643642    
## regionLong_Beach_VA  0.120733   0.435716   0.277 0.781709    
## regionSwitzerland    3.843320   1.162368   3.306 0.000945 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 710.13  on 530  degrees of freedom
## Residual deviance: 427.45  on 517  degrees of freedom
## AIC: 455.45
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>(author= Thomas Tang) Significant Predictors (p-value &lt; 0.05): sex
(1.409350): <em>Being male increases the log-odds of heart disease
significantly. </em>Odds Ratio: exp(1.469)=4.34 → Males are 4.34 times
more likely to have heart disease than females.</p>
<p>cp (chest pain): <em>Higher chest pain levels increase the log-odds
of heart disease. </em>Odds Ratio: exp(0.668)=1.95 → A strong predictor
for heart disease.</p>
<p>thalach (max heart rate achieved): <em>Higher heart rates decrease
the log-odds of heart disease. </em>Odds Ratio: exp(−0.014)=0.986,
suggesting a protective effect. exang (exercise-induced angina):
<em>Presence of exercise-induced angina increases the odds of heart
disease. </em>Odds Ratio: exp(0.899)=2.46. oldpeak (ST depression):
<em>Higher ST depression values significantly increase the odds of heart
disease. </em>Odds Ratio: exp(0.669)=1.95. regionSwitzerland:
<em>Patients from Switzerland have significantly higher odds of heart
disease compared to the reference region (Cleveland). </em>Odds Ratio:
exp(3.843)=46.64, suggesting a strong regional effect.</p>
<p>Non-Significant Predictors (p-value &gt; 0.05): age, trestbps
(resting blood pressure), chol (cholesterol), fbs (fasting blood sugar),
restecg (resting ECG results), regionHungarian, regionLong_Beach_VA.</p>
<p>Regional Effects: *Patients from Switzerland have much higher odds of
heart disease compared to Cleveland, Hungarian and Long Beach VA.</p>
<p>separated by region:</p>
<pre class="r"><code>cleveland_data &lt;- cleaned_data %&gt;% filter(region == &quot;Cleveland&quot;)
hungarian_data &lt;- cleaned_data %&gt;% filter(region == &quot;Hungarian&quot;)
long_beach_data &lt;- cleaned_data %&gt;% filter(region == &quot;Long_Beach_VA&quot;)
switzerland_data &lt;- cleaned_data %&gt;% filter(region == &quot;Switzerland&quot;)

cleveland_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                       restecg + thalach + exang + oldpeak,
                       data = cleveland_data, family = binomial)
hungarian_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                       restecg + thalach + exang + oldpeak,
                       data = hungarian_data, family = binomial)
long_beach_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                        restecg + thalach + exang + oldpeak,
                        data = long_beach_data, family = binomial)
switzerland_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                         restecg + thalach + exang + oldpeak,
                         data = switzerland_data, family = binomial)</code></pre>
<pre><code>## Warning: glm.fit:算法没有聚合

## Warning: glm.fit:拟合概率算出来是数值零或一</code></pre>
<pre class="r"><code>extract_results &lt;- function(model, region) {
  coefficients &lt;- summary(model)$coefficients
  data.frame(
    Region = region,
    Variable = rownames(coefficients),
    Estimate = coefficients[, &quot;Estimate&quot;],
    Std_Error = coefficients[, &quot;Std. Error&quot;],
    P_Value = coefficients[, &quot;Pr(&gt;|z|)&quot;]
  )
}

# Extract results for each region
cleveland_results &lt;- extract_results(cleveland_model, &quot;Cleveland&quot;)
hungarian_results &lt;- extract_results(hungarian_model, &quot;Hungarian&quot;)
long_beach_results &lt;- extract_results(long_beach_model, &quot;Long_Beach_VA&quot;)
switzerland_results &lt;- extract_results(switzerland_model, &quot;Switzerland&quot;)
regional_results &lt;- bind_rows(cleveland_results, hungarian_results,
                               long_beach_results, switzerland_results)

# View the consolidated results
regional_results</code></pre>
<pre><code>##                         Region    Variable      Estimate    Std_Error
## (Intercept)...1      Cleveland (Intercept) -6.409349e+00 2.366230e+00
## age...2              Cleveland         age  2.305274e-02 2.078413e-02
## sex...3              Cleveland         sex  1.914565e+00 3.963041e-01
## cp...4               Cleveland          cp  8.005256e-01 1.784567e-01
## trestbps...5         Cleveland    trestbps  1.933797e-02 9.694877e-03
## chol...6             Cleveland        chol  5.199003e-03 3.249779e-03
## fbs...7              Cleveland         fbs -1.801118e-01 4.422953e-01
## restecg...8          Cleveland     restecg  2.186373e-01 1.622814e-01
## thalach...9          Cleveland     thalach -2.566683e-02 9.002007e-03
## exang...10           Cleveland       exang  1.016419e+00 3.608669e-01
## oldpeak...11         Cleveland     oldpeak  5.908489e-01 1.596412e-01
## (Intercept)...12     Hungarian (Intercept)  9.770581e-01 5.205882e+00
## age...13             Hungarian         age -8.168691e-02 5.314665e-02
## sex...14             Hungarian         sex  1.356069e+00 6.866089e-01
## cp...15              Hungarian          cp  7.316002e-01 3.775361e-01
## trestbps...16        Hungarian    trestbps  3.011956e-02 1.858612e-02
## chol...17            Hungarian        chol  8.921158e-04 5.398937e-03
## fbs...18             Hungarian         fbs  1.801468e+01 1.879517e+03
## restecg...19         Hungarian     restecg -6.322004e-01 7.733075e-01
## thalach...20         Hungarian     thalach -3.876461e-02 1.929788e-02
## exang...21           Hungarian       exang  1.502774e-01 7.325074e-01
## oldpeak...22         Hungarian     oldpeak  9.291678e-01 5.364974e-01
## (Intercept)...23 Long_Beach_VA (Intercept) -1.366177e-01 4.132572e+00
## age...24         Long_Beach_VA         age  1.796714e-02 4.359182e-02
## sex...25         Long_Beach_VA         sex  4.559604e-01 1.494038e+00
## cp...26          Long_Beach_VA          cp -2.887811e-02 4.302410e-01
## trestbps...27    Long_Beach_VA    trestbps -1.079782e-02 1.676406e-02
## chol...28        Long_Beach_VA        chol -7.260783e-04 2.764071e-03
## fbs...29         Long_Beach_VA         fbs  1.762377e-01 7.424995e-01
## restecg...30     Long_Beach_VA     restecg -3.073046e-01 4.673256e-01
## thalach...31     Long_Beach_VA     thalach  1.472768e-03 1.510586e-02
## exang...32       Long_Beach_VA       exang  1.774583e+00 8.143903e-01
## oldpeak...33     Long_Beach_VA     oldpeak  4.119425e-01 3.761805e-01
## (Intercept)...34   Switzerland (Intercept) -4.864726e+02 3.401466e+05
## age...35           Switzerland         age -1.761182e+01 6.992787e+03
## sex...36           Switzerland         sex  1.400754e+02 1.953051e+05
## cp...37            Switzerland          cp  2.696132e+00 1.203799e+04
## trestbps...38      Switzerland    trestbps  6.806273e+00 2.736445e+03
## fbs...39           Switzerland         fbs -8.133709e+01 1.467447e+05
## restecg...40       Switzerland     restecg  9.072661e+01 1.413873e+05
## thalach...41       Switzerland     thalach  5.029885e+00 1.969977e+03
## exang...42         Switzerland       exang  2.544557e+02 1.604791e+05
## oldpeak...43       Switzerland     oldpeak -1.201417e+02 8.414472e+04
##                       P_Value
## (Intercept)...1  6.755232e-03
## age...2          2.673649e-01
## sex...3          1.358144e-06
## cp...4           7.263202e-06
## trestbps...5     4.608011e-02
## chol...6         1.096425e-01
## fbs...7          6.838460e-01
## restecg...8      1.778924e-01
## thalach...9      4.354982e-03
## exang...10       4.853456e-03
## oldpeak...11     2.146612e-04
## (Intercept)...12 8.511248e-01
## age...13         1.242909e-01
## sex...14         4.826551e-02
## cp...15          5.264416e-02
## trestbps...16    1.051161e-01
## chol...17        8.687557e-01
## fbs...18         9.923526e-01
## restecg...19     4.136269e-01
## thalach...20     4.456365e-02
## exang...21       8.374512e-01
## oldpeak...22     8.328873e-02
## (Intercept)...23 9.736277e-01
## age...24         6.802165e-01
## sex...25         7.602240e-01
## cp...26          9.464856e-01
## trestbps...27    5.195072e-01
## chol...28        7.927939e-01
## fbs...29         8.123796e-01
## restecg...30     5.108072e-01
## thalach...31     9.223321e-01
## exang...32       2.932927e-02
## oldpeak...33     2.734877e-01
## (Intercept)...34 9.988589e-01
## age...35         9.979905e-01
## sex...36         9.994277e-01
## cp...37          9.998213e-01
## trestbps...38    9.980154e-01
## fbs...39         9.995578e-01
## restecg...40     9.994880e-01
## thalach...41     9.979628e-01
## exang...42       9.987349e-01
## oldpeak...43     9.988608e-01</code></pre>
<pre class="r"><code>male_data &lt;- cleaned_data %&gt;% filter(sex == 1)
female_data &lt;- cleaned_data %&gt;% filter(sex == 0)

male_model &lt;- glm(num ~ age + cp + trestbps + chol + fbs +
                  restecg + thalach + exang + oldpeak,
                  data = male_data, family = binomial)
female_model &lt;- glm(num ~ age + cp + trestbps + chol + fbs +
                    restecg + thalach + exang + oldpeak,
                    data = female_data, family = binomial)
# Function to extract model results
extract_results &lt;- function(model, gender) {
  coefficients &lt;- summary(model)$coefficients
  data.frame(
    Gender = gender,
    Variable = rownames(coefficients),
    Estimate = coefficients[, &quot;Estimate&quot;],
    Std_Error = coefficients[, &quot;Std. Error&quot;],
    P_Value = coefficients[, &quot;Pr(&gt;|z|)&quot;]
  )
}

# Extract results for males and females
male_results &lt;- extract_results(male_model, &quot;Male&quot;)
female_results &lt;- extract_results(female_model, &quot;Female&quot;)
gender_results &lt;- bind_rows(male_results, female_results)</code></pre>
<p>Males Significant Predictors (p-value &lt; 0.05):</p>
<p>Chest Pain (cp) (β=0.624, p&lt;0.001): Higher chest pain levels
increase the odds of heart disease. Odds Ratio= exp(0.624)=1.87. Max
Heart Rate (thalach) (β=−0.026, p&lt;0.001): Higher maximum heart rates
reduce the odds of heart disease. Odds Ratio= exp(−0.026)=0.974
(protective effect). Exercise-Induced Angina (exang) (β=0.717, p=0.025):
Presence of exercise-induced angina significantly increases the odds of
heart disease. Odds Ratio=exp(0.717)=2.05. ST Depression (oldpeak)
(β=0.569, p&lt;0.001): Higher ST depression significantly increases the
odds of heart disease. Odds Ratio = exp(0.569)=1.77.</p>
<p>Non-Significant Predictors: age, trestbps, chol, fbs, restecg.</p>
<p>Females Significant Predictors (p-value &lt; 0.05):</p>
<p>Chest Pain (cp) (β=0.927, p=0.006): Stronger effect than males. Odds
Ratio = exp(0.927)=2.53. Exercise-Induced Angina (exang) (β=1.144,
p=0.037): Stronger effect than males. Odds Ratio = exp(1.144)=3.14. ST
Depression (oldpeak) (β=0.677,p=0.020): Similar effect to males. Odds
Ratio = exp(0.677)=1.97. Non-Significant Predictors: age, trestbps,
chol, fbs, restecg, thalach.</p>
<p>Conclusions Predictors for Both Genders: cp, exang, and oldpeak are
significant predictors for both males and females.</p>
<p>Gender-Specific Differences: Stronger effects of cp and exang in
females suggest potential gender-specific diagnostic markers for heart
disease.</p>
<p>(author: Yonghao YU)</p>
</div>
<div id="try-random-forest-classifier" class="section level2">
<h2>Try Random Forest Classifier!</h2>
<div id="a-brief-intro-to-random-forest-algorithm"
class="section level3">
<h3>A brief intro to Random Forest Algorithm</h3>
<p>Random Forest is an ensemble learning algorithm used for
classification and regression tasks. It builds multiple decision trees
using bootstrap sampling (random subsets of data) and selects features
randomly at each split to increase diversity. Each tree predicts
independently, and the final output is determined by majority voting
(classification) or averaging (regression). Random Forest is robust to
overfitting, handles high-dimensional data well, and provides feature
importance scores.</p>
</div>
<div
id="first-construct-the-model-with-all-predictors-and-then-show-the-feature-importance-trends-the-trend-is-descending-according-to-the-meandecreaseaccuracy"
class="section level3">
<h3>First, construct the model with all predictors and then show the
feature importance trends (the trend is descending according to the
MeanDecreaseAccuracy)!</h3>
<p>author: Yonghao YU</p>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Warning: 程序包&#39;caret&#39;是用R版本4.4.2 来建造的

## 载入需要的程序包：lattice

## 
## 载入程序包：&#39;caret&#39;

## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<pre class="r"><code>library(randomForest)</code></pre>
<pre><code>## Warning: 程序包&#39;randomForest&#39;是用R版本4.4.2 来建造的

## randomForest 4.7-1.2

## Type rfNews() to see new features/changes/bug fixes.

## 
## 载入程序包：&#39;randomForest&#39;

## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine

## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre class="r"><code># drop out the variable &quot;ca&quot; and &quot;thal&quot; which are have so many missing values inside
variables = c(&quot;cp&quot;, &quot;age&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;, &quot;num&quot;, &quot;restecg&quot;, &quot;fbs&quot;,&quot;trestbps&quot;,&quot;region&quot;, &quot;slope&quot;, &quot;exang&quot;)
data = combined_data_two[, variables]        
data$num = as.factor(data$num)

# check and deal with missing data
if (any(is.na(data))) {
  print(&quot;Missing value detected&quot;)
  data = na.omit(data)
  print(&quot;Missing data have been deleted&quot;)
}
# split the dataset into training and testing datasets
set.seed(42)
trainIndex = createDataPartition(data$num, p = 0.8, list = FALSE)
trainData = data[trainIndex, ]
testData = data[-trainIndex, ]

# Construct the random forest model
rf_model = randomForest(num ~ ., data = trainData, importance = TRUE)
rf_pred = predict(rf_model, testData)
rf_conf_matrix = confusionMatrix(rf_pred, testData$num)

# Then we show the feature importance trends(The trend is descending according to the MeanDecreaseAccuracy)
var_imp = importance(rf_model)
var_imp_df = as.data.frame(var_imp)
var_imp_df$Variable = rownames(var_imp_df)
rownames(var_imp_df) = NULL
var_imp_df = var_imp_df[order(var_imp_df$MeanDecreaseAccuracy, decreasing = TRUE), ]
ggplot(var_imp_df, aes(x = reorder(Variable, -MeanDecreaseAccuracy))) +
  geom_line(aes(y = MeanDecreaseAccuracy, group = 1, color = &quot;MeanDecreaseAccuracy&quot;)) +
  geom_point(aes(y = MeanDecreaseAccuracy, color = &quot;MeanDecreaseAccuracy&quot;)) +
  geom_line(aes(y = MeanDecreaseGini, group = 1, color = &quot;MeanDecreaseGini&quot;)) +
  geom_point(aes(y = MeanDecreaseGini, color = &quot;MeanDecreaseGini&quot;)) +
  labs(title = &quot;Feature Importance Trends&quot;,
       x = &quot;Features&quot;,
       y = &quot;Importance&quot;,
       color = &quot;Metric&quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1))</code></pre>
<p><img
src="modelling_files/figure-gfm/unnamed-chunk-9-1.png" /><!-- --></p>
<p>Then we ranked the predictors descendingly based on the
MeanDecreaseAccuracy which measures the decrease in overall model
accuracy when the variable is permuted. And we show them in a line plot!
Based on the MeanDecreaseAccuracy and MeanDecreaseGini, we can drop out
restecg and fbs predictors that have relatively small impact on our
prediction. And then we can focus on the first seven predictors that
have more impact on our prediction results!</p>
</div>
<div
id="then-drop-unsignificant-features-and-build-the-model-again-show-the-confusion-matrix"
class="section level3">
<h3>Then, drop unsignificant features and build the model again, show
the confusion matrix</h3>
<pre class="r"><code>variables2 = c(&quot;cp&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;, &quot;num&quot;,&quot;trestbps&quot;,&quot;region&quot;,&quot;slope&quot;, &quot;exang&quot;,&quot;age&quot;)
data2 = combined_data_two[, variables2]
data2$num = as.factor(data2$num)

set.seed(50)

trainIndex2 = createDataPartition(data2$num, p = 0.8, list = FALSE)
trainData2 = data2[trainIndex2, ]
testData2 = data2[-trainIndex2, ]

# Construct the random forest model and evaluate the model results
rf_model2 = randomForest(num ~ ., data = trainData2, importance = TRUE)
rf_pred2 = predict(rf_model2, testData2)
rf_conf_matrix2 = confusionMatrix(rf_pred2, testData2$num)

cm = rf_conf_matrix2$table
cm_df = as.data.frame(cm)
colnames(cm_df) = c(&quot;Predicted&quot;, &quot;Actual&quot;, &quot;Frequency&quot;)
cm_df$Proportion = cm_df$Frequency / sum(cm_df$Frequency)

ggplot(data = cm_df, aes(x = Predicted, y = Actual, fill = Proportion)) +
  geom_tile(color = &quot;white&quot;) +
  scale_fill_gradient(low = &quot;white&quot;, high = &quot;steelblue&quot;, name = &quot;Proportion&quot;) +
  geom_text(aes(label = Frequency), color = &quot;black&quot;, size = 5) +  # Add frequency labels
  labs(
    title = &quot;Confusion Matrix&quot;,
    x = &quot;Predicted Class&quot;,
    y = &quot;Actual Class&quot;
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = &quot;bold&quot;, hjust = 0.5),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )</code></pre>
<p><img
src="modelling_files/figure-gfm/unnamed-chunk-10-1.png" /><!-- --></p>
</div>
<div id="extracting-detailed-statistics-from-the-confusion-matrix"
class="section level3">
<h3>Extracting detailed statistics from the confusion matrix</h3>
<pre class="r"><code>stats = rf_conf_matrix2$overall
class_stats = rf_conf_matrix2$byClass

# Displaying overall statistics
cat(&quot;Overall Statistics:\n&quot;)</code></pre>
<pre><code>## Overall Statistics:</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Accuracy: %.4f\n&quot;, stats[&quot;Accuracy&quot;]))</code></pre>
<pre><code>## Accuracy: 0.8095</code></pre>
<pre class="r"><code>cat(sprintf(&quot;95%% CI: (%.4f, %.4f)\n&quot;, stats[&quot;AccuracyLower&quot;], stats[&quot;AccuracyUpper&quot;]))</code></pre>
<pre><code>## 95% CI: (0.7213, 0.8796)</code></pre>
<pre class="r"><code>cat(sprintf(&quot;No Information Rate: %.4f\n&quot;, stats[&quot;AccuracyNull&quot;]))</code></pre>
<pre><code>## No Information Rate: 0.6095</code></pre>
<pre class="r"><code>cat(sprintf(&quot;P-Value [Acc &gt; NIR]: %.6f\n&quot;, stats[&quot;AccuracyPValue&quot;]))</code></pre>
<pre><code>## P-Value [Acc &gt; NIR]: 0.000009</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Kappa: %.4f\n&quot;, stats[&quot;Kappa&quot;]))</code></pre>
<pre><code>## Kappa: 0.6067</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Mcnemar&#39;s Test P-Value: %.4f\n\n&quot;, stats[&quot;McnemarPValue&quot;]))</code></pre>
<pre><code>## Mcnemar&#39;s Test P-Value: 0.5023</code></pre>
<pre class="r"><code># Displaying class-specific statistics
cat(&quot;Class-Specific Statistics:\n&quot;)</code></pre>
<pre><code>## Class-Specific Statistics:</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Sensitivity: %.4f\n&quot;, class_stats[&quot;Sensitivity&quot;]))</code></pre>
<pre><code>## Sensitivity: 0.8049</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Specificity: %.4f\n&quot;, class_stats[&quot;Specificity&quot;]))</code></pre>
<pre><code>## Specificity: 0.8125</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Pos Pred Value: %.4f\n&quot;, class_stats[&quot;Pos Pred Value&quot;]))</code></pre>
<pre><code>## Pos Pred Value: 0.7333</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Neg Pred Value: %.4f\n&quot;, class_stats[&quot;Neg Pred Value&quot;]))</code></pre>
<pre><code>## Neg Pred Value: 0.8667</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Prevalence: %.4f\n&quot;, class_stats[&quot;Prevalence&quot;]))</code></pre>
<pre><code>## Prevalence: 0.3905</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Detection Rate: %.4f\n&quot;, class_stats[&quot;Detection Rate&quot;]))</code></pre>
<pre><code>## Detection Rate: 0.3143</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Detection Prevalence: %.4f\n&quot;, class_stats[&quot;Detection Prevalence&quot;]))</code></pre>
<pre><code>## Detection Prevalence: 0.4286</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Balanced Accuracy: %.4f\n&quot;, class_stats[&quot;Balanced Accuracy&quot;]))</code></pre>
<pre><code>## Balanced Accuracy: 0.8087</code></pre>
<p>From the model we can observe the following things: 1. The model
correctly classified 80.95% of the instances. 2. The true accuracy is
expected to fall 95% of the time in (0.7213, 0.8796) 3. The no
information rate is 0.6095 which is less than the accuracy rate (p-value
also indicate this), indicating the model we built actually capture some
significant features. 4. The Kappa is 0.6067 which is in the range <span
class="math display">\[0.6,0.8\]</span>, which indicate our classifier
achieves relatively high level of classification 5. High sensitivity
(0.8049) indicates good identification of positives. 6. High specificity
(0.8125) indicates good identification of negatives. 7. Balanced
accuracy (0.8087) suggests the model balances its performance across
both classes well.</p>
</div>
<div id="compute-r-squared-and-rmse-for-the-classification-model"
class="section level3">
<h3>Compute R-squared and RMSE for the classification model</h3>
<pre class="r"><code>rf_pred_numeric = as.numeric(rf_pred2)
test_actual_numeric = as.numeric(testData2$num)

mean_actual = mean(test_actual_numeric)

SST = sum((test_actual_numeric - mean_actual)^2)

SSE = sum((test_actual_numeric - rf_pred_numeric)^2)

R_squared = 1 - (SSE / SST)

# Calculate RMSE
rmse = sqrt(mean((rf_pred_numeric - test_actual_numeric)^2))

# Print the results
cat(&quot;RMSE of the model is:&quot;, rmse, &quot;\n&quot;)</code></pre>
<pre><code>## RMSE of the model is: 0.4364358</code></pre>
<pre class="r"><code>cat(&quot;R-squared: &quot;, R_squared, &quot;\n&quot;)</code></pre>
<pre><code>## R-squared:  0.1996951</code></pre>
<p>author: Yonghao YU ### Then investigate the AUC value and ROC curve
to assess the model’s ability</p>
<pre class="r"><code>library(pROC)</code></pre>
<pre><code>## Warning: 程序包&#39;pROC&#39;是用R版本4.4.2 来建造的

## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.

## 
## 载入程序包：&#39;pROC&#39;

## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre class="r"><code># Generate AUC value
rf_prob = predict(rf_model2, testData2, type = &quot;prob&quot;)
roc_curve = roc(testData2$num, rf_prob[, 2], levels = rev(levels(testData2$num)))</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>auc_value = auc(roc_curve)
roc_data = data.frame(
  FPR = 1 - roc_curve$specificities,
  TPR = roc_curve$sensitivities
)

# Plot the ROC curve
ggplot(data = roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = &quot;blue&quot;, size = 1) +
  geom_abline(linetype = &quot;dashed&quot;, color = &quot;gray&quot;) +
  labs(
    title = &quot;ROC Curve for Random Forest Model&quot;,
    x = &quot;False Positive Rate (1 - Specificity)&quot;,
    y = &quot;True Positive Rate (Sensitivity)&quot;,
    subtitle = paste(&quot;AUC =&quot;, round(auc_value, 2))
  ) +
  theme_minimal()</code></pre>
<pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<p><img
src="modelling_files/figure-gfm/unnamed-chunk-13-1.png" /><!-- --></p>
<p>The AUC value of 0.85 indicates that the Random Forest model performs
well in distinguishing between positive and negative classes.
Specifically, there is an 85% chance that the model will rank a randomly
chosen positive instance higher than a negative one. This reflects our
model reaches good discrimination.</p>
<p>The ROC curve shows the trade-off between the True Positive Rate
(Sensitivity) and the False Positive Rate (1 - Specificity) at various
thresholds. The curve is well above the diagonal which represent random
guessing, confirming the model performs better than random guessing. The
initial steep rise indicates that the model achieves high sensitivity
with a relatively low false positive rate, which is desirable. However,
as the false positive rate increases, the curve flattens, highlighting
diminishing returns in improving sensitivity further.</p>
<p>author: Yonghao YU ### Last, simulate the prediction which predicts
the num with the new data based on the model we built!</p>
<p>author: Yonghao YU</p>
<pre class="r"><code># construct a new dataframe which includes new data
new_data = data.frame(
  age = c(63,39,62,34),
  sex = c(1, 1, 1, 0),
  cp = c(1, 2, 4, 1),
  trestbps = c(145, 120, 110, 125),
  thalach = c(150, 160, 120, 140),
  exang = c(0, 1, 1, 0),
  oldpeak = c(2.3, 1, 0.5, 2),
  slope = c(3, 2, 2, 1),
  region = c(1, 2, 3, 4)
)
# predict the results based on the model we have trained
predicted_num = predict(rf_model2, new_data)
print(&quot;The prediction result is：&quot;)</code></pre>
<pre><code>## [1] &quot;The prediction result is：&quot;</code></pre>
<pre class="r"><code>print(data.frame(new_data, Predicted_num = predicted_num))</code></pre>
<pre><code>##   age sex cp trestbps thalach exang oldpeak slope region Predicted_num
## 1  63   1  1      145     150     0     2.3     3      1             0
## 2  39   1  2      120     160     1     1.0     2      2             0
## 3  62   1  4      110     120     1     0.5     2      3             1
## 4  34   0  1      125     140     0     2.0     1      4             0</code></pre>
<p>From the results, we can see that the model can generate some results
based on the predictor values we put in!</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
