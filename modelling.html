<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Modelling</title>

<script src="site_libs/header-attrs-2.28/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="final_report.html">Report</a>
</li>
<li>
  <a href="eda.html">EDA</a>
</li>
<li>
  <a href="modelling.html">Modelling</a>
</li>
<li>
  <a href="https://ys3875.shinyapps.io/dashboardcopy/">Dashboard</a>
</li>
<li>
  <a href="https://github.com/bhk0216/p8105_final_project">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Modelling</h1>

</div>


<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.4     ✔ readr     2.1.5
## ✔ forcats   1.0.0     ✔ stringr   1.5.1
## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
## ✔ purrr     1.0.2     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
<pre class="r"><code>library(janitor)</code></pre>
<pre><code>## 
## 载入程序包：&#39;janitor&#39;
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     chisq.test, fisher.test</code></pre>
<pre class="r"><code>library(car)</code></pre>
<pre><code>## 载入需要的程序包：carData
## 
## 载入程序包：&#39;car&#39;
## 
## The following object is masked from &#39;package:dplyr&#39;:
## 
##     recode
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     some</code></pre>
<pre class="r"><code>library(skimr)
library(broom)</code></pre>
<p>Interpreting num: The values for <code>num</code> represent the
degree of narrowing in the coronary arteries: 0: No disease (&lt; 50%
diameter narrowing). 1-4: Increasing severity of disease (&gt; 50%
diameter narrowing, with different severities).</p>
<p>For convenience, this variable will binarized: 0: No heart disease
(value 0 in num). 1: Presence of heart disease (values 1-4 in num).</p>
<p>*but if we want to analyze the severity of heart disease num will be
treated as a categorical variable. example code: cleaned_data &lt;- data
|&gt; mutate(num = factor(num, levels = c(0, 1, 2, 3, 4), labels = c(“No
Disease”, “Mild”, “Moderate”, “Severe”, “Very Severe”)))</p>
<pre class="r"><code>cleveland &lt;- read_csv(&quot;./data/cleveland.csv&quot;, na = &quot;?&quot;) |&gt; 
  clean_names() |&gt; 
  mutate(num = if_else(num == 0, 0, 1)) # Binarize the `num` variable: 0 = no heart disease, 1 = heart disease</code></pre>
<pre><code>## Rows: 303 Columns: 14
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># |&gt; drop_na() Removes rows with any missing values (optional, adjust as needed)

hungary = read_csv(&quot;./data/hungarian.csv&quot;, na = &quot;?&quot;) |&gt; 
  clean_names() |&gt; 
  mutate(num = if_else(num == 0, 0, 1))</code></pre>
<pre><code>## Rows: 294 Columns: 14
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># |&gt; drop_na() Removes rows with any missing values (optional, adjust as needed)

long_beach = read_csv(&quot;./data/long_beach_va.csv&quot;, na = &quot;?&quot;) |&gt; 
  clean_names() |&gt; 
  mutate(num = if_else(num == 0, 0, 1))</code></pre>
<pre><code>## Rows: 200 Columns: 14
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># |&gt; drop_na() Removes rows with any missing values (optional, adjust as needed)

switzerland = read_csv(&quot;./data/switzerland.csv&quot;, na = &quot;?&quot;) |&gt; 
  clean_names() |&gt; 
  mutate(num = if_else(num == 0, 0, 1))</code></pre>
<pre><code>## Rows: 123 Columns: 14
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># |&gt; drop_na() Removes rows with any missing values (optional, adjust as needed) </code></pre>
<pre class="r"><code>cor(cleveland$chol, cleveland$num, use = &quot;complete.obs&quot;)</code></pre>
<pre><code>## [1] 0.08516361</code></pre>
<div id="variable-selection" class="section level1">
<h1>Variable Selection</h1>
<div id="data-preprocessing" class="section level3">
<h3>Data Preprocessing</h3>
<pre class="r"><code>cleveland$region = &quot;Cleveland&quot;
hungary$region = &quot;Hungarian&quot;
long_beach$region = &quot;Long_Beach_VA&quot;
switzerland$region = &quot;Switzerland&quot;
combined_data_one = bind_rows(cleveland, hungary, long_beach, switzerland)

colnames(combined_data_one) = c(&quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;fbs&quot;,
                             &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, &quot;slope&quot;,
                             &quot;ca&quot;, &quot;thal&quot;, &quot;num&quot;, &quot;region&quot;)
combined_data_two = combined_data_one |&gt;
  mutate(region = case_when(
    region == &quot;Cleveland&quot; ~ 1,
    region == &quot;Hungarian&quot; ~ 2,
    region == &quot;Long_Beach_VA&quot; ~ 3,
    region == &quot;Switzerland&quot; ~ 4,
  )) |&gt;
  select(-thal,-ca) |&gt;
  drop_na()
case_data = combined_data_two |&gt;
  filter(num == 1)
control_data = combined_data_two |&gt;
  filter(num == 0)
print(case_data)</code></pre>
<pre><code>## # A tibble: 324 × 13
##      age   sex    cp trestbps  chol   fbs restecg thalach exang oldpeak slope
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1    67     1     4      160   286     0       2     108     1     1.5     2
##  2    67     1     4      120   229     0       2     129     1     2.6     2
##  3    62     0     4      140   268     0       2     160     0     3.6     3
##  4    63     1     4      130   254     0       2     147     0     1.4     2
##  5    53     1     4      140   203     1       2     155     1     3.1     3
##  6    56     1     3      130   256     1       2     142     1     0.6     2
##  7    48     1     2      110   229     0       0     168     0     1       3
##  8    58     1     2      120   284     0       2     160     0     1.8     2
##  9    58     1     3      132   224     0       2     173     0     3.2     1
## 10    60     1     4      130   206     0       2     132     1     2.4     2
## # ℹ 314 more rows
## # ℹ 2 more variables: num &lt;dbl&gt;, region &lt;dbl&gt;</code></pre>
<pre class="r"><code>print(control_data)</code></pre>
<pre><code>## # A tibble: 207 × 13
##      age   sex    cp trestbps  chol   fbs restecg thalach exang oldpeak slope
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1    63     1     1      145   233     1       2     150     0     2.3     3
##  2    37     1     3      130   250     0       0     187     0     3.5     3
##  3    41     0     2      130   204     0       2     172     0     1.4     1
##  4    56     1     2      120   236     0       0     178     0     0.8     1
##  5    57     0     4      120   354     0       0     163     1     0.6     1
##  6    57     1     4      140   192     0       0     148     0     0.4     2
##  7    56     0     2      140   294     0       2     153     0     1.3     2
##  8    44     1     2      120   263     0       0     173     0     0       1
##  9    52     1     3      172   199     1       0     162     0     0.5     1
## 10    57     1     3      150   168     0       0     174     0     1.6     1
## # ℹ 197 more rows
## # ℹ 2 more variables: num &lt;dbl&gt;, region &lt;dbl&gt;</code></pre>
<pre class="r"><code>print(combined_data_two)</code></pre>
<pre><code>## # A tibble: 531 × 13
##      age   sex    cp trestbps  chol   fbs restecg thalach exang oldpeak slope
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1    63     1     1      145   233     1       2     150     0     2.3     3
##  2    67     1     4      160   286     0       2     108     1     1.5     2
##  3    67     1     4      120   229     0       2     129     1     2.6     2
##  4    37     1     3      130   250     0       0     187     0     3.5     3
##  5    41     0     2      130   204     0       2     172     0     1.4     1
##  6    56     1     2      120   236     0       0     178     0     0.8     1
##  7    62     0     4      140   268     0       2     160     0     3.6     3
##  8    57     0     4      120   354     0       0     163     1     0.6     1
##  9    63     1     4      130   254     0       2     147     0     1.4     2
## 10    53     1     4      140   203     1       2     155     1     3.1     3
## # ℹ 521 more rows
## # ℹ 2 more variables: num &lt;dbl&gt;, region &lt;dbl&gt;</code></pre>
</div>
<div id="for-continuous-case" class="section level3">
<h3>For Continuous case</h3>
<p>For continuous variables, we use mean and standard deviation (std) to
describe the distribution in overall samples, samples of control(num =
0), and samples of case(num = 1). Then, we use t-test to examine whether
the means of these variables are significantly different between case
group and control group (significance level = 0.05).</p>
<pre class="r"><code># 1. Mean and Std for Continuous Variables (Overall)
list_conti_all = list(
  age = combined_data_two$age,
  trestbps = combined_data_two$trestbps,
  chol = combined_data_two$chol,
  thalach = combined_data_two$thalach,
  oldpeak = combined_data_two$oldpeak
) |&gt; 
  lapply(na.omit) 

mean_all = sapply(list_conti_all, mean) |&gt; 
  as.data.frame()|&gt;
  setNames(&quot;Overall Mean&quot;)

std_all = sapply(list_conti_all, sd) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Overall Std&quot;)

# 2. p-value of t-test for Continuous Variables
t_test = function(variable) {
  t_test_result = t.test(combined_data_two[[variable]] ~ combined_data_two$num)
  return(data.frame(
    variable = variable,
    p_value = t_test_result$p.value
  ))
}

p_value = 
  lapply(c(&quot;age&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;), t_test) |&gt; 
  bind_rows() |&gt; 
  as.data.frame()

# 3. Mean and Std for Control Group
list_conti_control = list(
  age = control_data$age,
  trestbps = control_data$trestbps,
  chol = control_data$chol,
  thalach = control_data$thalach,
  oldpeak = control_data$oldpeak
) |&gt; 
  lapply(na.omit)

mean_control = sapply(list_conti_control, mean) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Control Mean&quot;)

std_control = sapply(list_conti_control, sd) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Control Std&quot;)

# 4. Mean and Std for Case Group
list_conti_case = list(
  age = case_data$age,
  trestbps = case_data$trestbps,
  chol = case_data$chol,
  thalach = case_data$thalach,
  oldpeak = case_data$oldpeak
) |&gt; 
  lapply(na.omit)

mean_case = sapply(list_conti_case, mean) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Case Mean&quot;)

std_case = sapply(list_conti_case, sd) |&gt; 
  as.data.frame() |&gt;
  setNames(&quot;Case Std&quot;)

conti_des_df =
  as.data.frame(cbind(mean_all, std_all, mean_control, std_control, mean_case, std_case, p_value))
conti_des_df = conti_des_df[, -grep(&quot;variable&quot;, colnames(conti_des_df))] |&gt; 
  knitr::kable(digits = 6)
conti_des_df</code></pre>
<table style="width:100%;">
<colgroup>
<col width="10%" />
<col width="14%" />
<col width="13%" />
<col width="14%" />
<col width="13%" />
<col width="12%" />
<col width="12%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Overall Mean</th>
<th align="right">Overall Std</th>
<th align="right">Control Mean</th>
<th align="right">Control Std</th>
<th align="right">Case Mean</th>
<th align="right">Case Std</th>
<th align="right">p_value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">age</td>
<td align="right">54.843691</td>
<td align="right">8.824069</td>
<td align="right">52.908213</td>
<td align="right">9.248788</td>
<td align="right">56.080247</td>
<td align="right">8.323177</td>
<td align="right">0.000074</td>
</tr>
<tr class="even">
<td align="left">trestbps</td>
<td align="right">133.406780</td>
<td align="right">18.969496</td>
<td align="right">129.734300</td>
<td align="right">16.322060</td>
<td align="right">135.753086</td>
<td align="right">20.158831</td>
<td align="right">0.000179</td>
</tr>
<tr class="odd">
<td align="left">chol</td>
<td align="right">216.854991</td>
<td align="right">99.014215</td>
<td align="right">237.043478</td>
<td align="right">68.313903</td>
<td align="right">203.956790</td>
<td align="right">112.615863</td>
<td align="right">0.000030</td>
</tr>
<tr class="even">
<td align="left">thalach</td>
<td align="right">138.463277</td>
<td align="right">25.833649</td>
<td align="right">152.758454</td>
<td align="right">22.958375</td>
<td align="right">129.330247</td>
<td align="right">23.329890</td>
<td align="right">0.000000</td>
</tr>
<tr class="odd">
<td align="left">oldpeak</td>
<td align="right">1.218456</td>
<td align="right">1.105150</td>
<td align="right">0.726087</td>
<td align="right">0.805741</td>
<td align="right">1.533025</td>
<td align="right">1.155598</td>
<td align="right">0.000000</td>
</tr>
</tbody>
</table>
<p>Based on the result, we can find that all five features are
significantly different between case and control.</p>
</div>
<div id="for-discrete-case" class="section level3">
<h3>For Discrete case</h3>
<p>For binary and categorical variables, we use count (n) and percentage
(pct) to describe the distribution in overall samples, samples of
control(num = 0), and samples of case(num = 1). Then, as the data meet
the assumption, we use chi-sq test to examine whether the distribution
of these variables are significantly different between case group and
control group (significance level = 0.05).</p>
<pre class="r"><code>list_cat_all = as.data.frame(list(
  sex = combined_data_two$sex,
  cp = combined_data_two$cp,
  fbs = combined_data_two$fbs,
  restecg = combined_data_two$restecg,
  exang = combined_data_two$exang,
  slope = combined_data_two$slope,
  region = combined_data_two$region
))

# 1. Overall Counts and Chi-Square Test
cat_vars = names(list_cat_all)

count_all_function = function(variable) {
  table_value = table(list_cat_all[[variable]], combined_data_two$num) 
  chi_sq_test = chisq.test(table_value)
  
  count = table(list_cat_all[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    n = as.numeric(count),
    pct = round(pct, 3),
    p_value = round(chi_sq_test$p.value, 3)
  )
  
  return(result_df)
}

cat_count_chisq = lapply(cat_vars, count_all_function) |&gt; 
  bind_rows()

# 2. Control Group Counts and Percentages
list_cat_ctrl = as.data.frame(list(
  sex = control_data$sex,
  cp = control_data$cp,
  fbs = control_data$fbs,
  restecg = control_data$restecg,
  exang = control_data$exang,
  slope = control_data$slope,
  region = control_data$region
))

cat_vars_ctrl = names(list_cat_ctrl)

count_ctrl_function = function(variable) {
  count = table(list_cat_ctrl[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    control_n = as.numeric(count),
    control_pct = round(pct, 3)
  )
  
  return(result_df)
}

cat_count_ctrl = lapply(cat_vars_ctrl, count_ctrl_function) |&gt; 
  bind_rows()

# 3. Case Group Counts and Percentages
list_cat_case = as.data.frame(list(
  sex = case_data$sex,
  cp = case_data$cp,
  fbs = case_data$fbs,
  restecg = case_data$restecg,
  exang = case_data$exang,
  slope = case_data$slope,
  region = case_data$region
))

cat_vars_case = names(list_cat_case)

count_case_function = function(variable) {
  count = table(list_cat_case[[variable]])
  total = sum(count)
  pct = count / total
  
  result_df = tibble(
    variable = rep(variable, length(count)),
    category = names(count),
    case_n = as.numeric(count),
    case_pct = round(pct, 3)
  )
  return(result_df)
}

cat_count_case = lapply(cat_vars_case, count_case_function) |&gt; 
  bind_rows()

# 4. Combine Results
final_cat_count = cat_count_chisq |&gt;
  left_join(cat_count_ctrl, by = c(&quot;variable&quot;, &quot;category&quot;)) |&gt;
  left_join(cat_count_case, by = c(&quot;variable&quot;, &quot;category&quot;))|&gt;
  knitr::kable(digits = 3)

final_cat_count</code></pre>
<table style="width:100%;">
<colgroup>
<col width="12%" />
<col width="12%" />
<col width="5%" />
<col width="8%" />
<col width="10%" />
<col width="13%" />
<col width="16%" />
<col width="9%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="left">category</th>
<th align="right">n</th>
<th align="right">pct</th>
<th align="right">p_value</th>
<th align="right">control_n</th>
<th align="right">control_pct</th>
<th align="right">case_n</th>
<th align="right">case_pct</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">sex</td>
<td align="left">0</td>
<td align="right">127</td>
<td align="right">0.239</td>
<td align="right">0.000</td>
<td align="right">87</td>
<td align="right">0.420</td>
<td align="right">40</td>
<td align="right">0.123</td>
</tr>
<tr class="even">
<td align="left">sex</td>
<td align="left">1</td>
<td align="right">404</td>
<td align="right">0.761</td>
<td align="right">0.000</td>
<td align="right">120</td>
<td align="right">0.580</td>
<td align="right">284</td>
<td align="right">0.877</td>
</tr>
<tr class="odd">
<td align="left">cp</td>
<td align="left">1</td>
<td align="right">30</td>
<td align="right">0.056</td>
<td align="right">0.000</td>
<td align="right">18</td>
<td align="right">0.087</td>
<td align="right">12</td>
<td align="right">0.037</td>
</tr>
<tr class="even">
<td align="left">cp</td>
<td align="left">2</td>
<td align="right">70</td>
<td align="right">0.132</td>
<td align="right">0.000</td>
<td align="right">51</td>
<td align="right">0.246</td>
<td align="right">19</td>
<td align="right">0.059</td>
</tr>
<tr class="odd">
<td align="left">cp</td>
<td align="left">3</td>
<td align="right">114</td>
<td align="right">0.215</td>
<td align="right">0.000</td>
<td align="right">80</td>
<td align="right">0.386</td>
<td align="right">34</td>
<td align="right">0.105</td>
</tr>
<tr class="even">
<td align="left">cp</td>
<td align="left">4</td>
<td align="right">317</td>
<td align="right">0.597</td>
<td align="right">0.000</td>
<td align="right">58</td>
<td align="right">0.280</td>
<td align="right">259</td>
<td align="right">0.799</td>
</tr>
<tr class="odd">
<td align="left">fbs</td>
<td align="left">0</td>
<td align="right">446</td>
<td align="right">0.840</td>
<td align="right">0.378</td>
<td align="right">178</td>
<td align="right">0.860</td>
<td align="right">268</td>
<td align="right">0.827</td>
</tr>
<tr class="even">
<td align="left">fbs</td>
<td align="left">1</td>
<td align="right">85</td>
<td align="right">0.160</td>
<td align="right">0.378</td>
<td align="right">29</td>
<td align="right">0.140</td>
<td align="right">56</td>
<td align="right">0.173</td>
</tr>
<tr class="odd">
<td align="left">restecg</td>
<td align="left">0</td>
<td align="right">297</td>
<td align="right">0.559</td>
<td align="right">0.000</td>
<td align="right">123</td>
<td align="right">0.594</td>
<td align="right">174</td>
<td align="right">0.537</td>
</tr>
<tr class="even">
<td align="left">restecg</td>
<td align="left">1</td>
<td align="right">73</td>
<td align="right">0.137</td>
<td align="right">0.000</td>
<td align="right">13</td>
<td align="right">0.063</td>
<td align="right">60</td>
<td align="right">0.185</td>
</tr>
<tr class="odd">
<td align="left">restecg</td>
<td align="left">2</td>
<td align="right">161</td>
<td align="right">0.303</td>
<td align="right">0.000</td>
<td align="right">71</td>
<td align="right">0.343</td>
<td align="right">90</td>
<td align="right">0.278</td>
</tr>
<tr class="even">
<td align="left">exang</td>
<td align="left">0</td>
<td align="right">267</td>
<td align="right">0.503</td>
<td align="right">0.000</td>
<td align="right">163</td>
<td align="right">0.787</td>
<td align="right">104</td>
<td align="right">0.321</td>
</tr>
<tr class="odd">
<td align="left">exang</td>
<td align="left">1</td>
<td align="right">264</td>
<td align="right">0.497</td>
<td align="right">0.000</td>
<td align="right">44</td>
<td align="right">0.213</td>
<td align="right">220</td>
<td align="right">0.679</td>
</tr>
<tr class="even">
<td align="left">slope</td>
<td align="left">1</td>
<td align="right">173</td>
<td align="right">0.326</td>
<td align="right">0.000</td>
<td align="right">119</td>
<td align="right">0.575</td>
<td align="right">54</td>
<td align="right">0.167</td>
</tr>
<tr class="odd">
<td align="left">slope</td>
<td align="left">2</td>
<td align="right">310</td>
<td align="right">0.584</td>
<td align="right">0.000</td>
<td align="right">76</td>
<td align="right">0.367</td>
<td align="right">234</td>
<td align="right">0.722</td>
</tr>
<tr class="even">
<td align="left">slope</td>
<td align="left">3</td>
<td align="right">48</td>
<td align="right">0.090</td>
<td align="right">0.000</td>
<td align="right">12</td>
<td align="right">0.058</td>
<td align="right">36</td>
<td align="right">0.111</td>
</tr>
<tr class="odd">
<td align="left">region</td>
<td align="left">1</td>
<td align="right">303</td>
<td align="right">0.571</td>
<td align="right">0.000</td>
<td align="right">164</td>
<td align="right">0.792</td>
<td align="right">139</td>
<td align="right">0.429</td>
</tr>
<tr class="even">
<td align="left">region</td>
<td align="left">2</td>
<td align="right">95</td>
<td align="right">0.179</td>
<td align="right">0.000</td>
<td align="right">27</td>
<td align="right">0.130</td>
<td align="right">68</td>
<td align="right">0.210</td>
</tr>
<tr class="odd">
<td align="left">region</td>
<td align="left">3</td>
<td align="right">87</td>
<td align="right">0.164</td>
<td align="right">0.000</td>
<td align="right">15</td>
<td align="right">0.072</td>
<td align="right">72</td>
<td align="right">0.222</td>
</tr>
<tr class="even">
<td align="left">region</td>
<td align="left">4</td>
<td align="right">46</td>
<td align="right">0.087</td>
<td align="right">0.000</td>
<td align="right">1</td>
<td align="right">0.005</td>
<td align="right">45</td>
<td align="right">0.139</td>
</tr>
</tbody>
</table>
<p>Based on the result, we can find that except fbs, the rest of all
other binary and categorical features are significantly different
between case and control.</p>
</div>
</div>
<div id="linear-regression" class="section level1">
<h1>Linear Regression</h1>
<p>Here we are prioritizing MLR over SLR, as the nature of heart disease
involves multiple interacting factors (e.g., age, chest pain, and
exercise-induced angina). Modeling a single variable may oversimplify
these relationships and fail to capture their combined effects. For
example, maximum heart rate (thalach) may interact with age or region,
making it more meaningful to include these interactions in a multiple
linear regression (MLR) model.</p>
<ol style="list-style-type: decimal">
<li>Comparing Diagnostic Factors for Heart Disease Across Regions
Explore whether certain diagnostic factors (e.g., blood pressure and
exercise-induced angina) are more predictive of heart disease in one
region compared to others.</li>
</ol>
<p>Due to the high number of missing values in the <code>chol</code>
column (particularly in Switzerland), alternative predictors should be
used for analysis. Include an interaction term between the diagnostic
factor and region to capture regional differences.</p>
<div id="checking-datasets" class="section level2">
<h2>checking datasets</h2>
<pre class="r"><code># Add numeric encoding for regions
region_data &lt;- combined_data_two %&gt;%
  mutate(region_factor = as.factor(region))

ggplot(region_data, aes(x = trestbps, y = num)) +
  geom_point(aes(color = region_factor), alpha = 0.6) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(group = region_factor)) +
  labs(title = &quot;Blood Pressure vs Heart Disease Status&quot;, 
       x = &quot;Blood Pressure (trestbps)&quot;, 
       y = &quot;Heart Disease (num)&quot;) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>ggplot(region_data, aes(x = exang, y = num)) +
  geom_point(aes(color = region_factor), alpha = 0.6) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(group = region_factor)) +
  labs(title = &quot;Exercise-Induced Angina vs Heart Disease Status&quot;, 
       x = &quot;Exercise-Induced Angina (exang)&quot;, 
       y = &quot;Heart Disease (num)&quot;) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre class="r"><code>ggplot(region_data, aes(x = factor(num))) +
  geom_bar(aes(fill = region_factor), position = &quot;dodge&quot;) +
  labs(title = &quot;Distribution of Heart Disease Status by Region&quot;, 
       x = &quot;Heart Disease Status (num)&quot;, 
       y = &quot;Count&quot;) +
  theme_minimal()</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-7-3.png" width="672" /></p>
<p>Scatterplots of Resting Blood Pressure (trestbps) show variability in
the relationship between trestbps and heart disease across regions,
justifying the inclusion of interaction terms. Plot for Exercise-Induced
Angina (exang) is consistent within regions but varying slopes between
regions highlight the importance of interaction effects. Plot of
prevalence of Heart Disease (num) shows substantial differences in
baseline heart disease prevalence across regions further support
modeling regional effects.</p>
</div>
<div id="hypothesis-mlr" class="section level2">
<h2>Hypothesis (MLR):</h2>
<p>Null Hypothesis (<span class="math inline">\(H_0\)</span>: Blood
pressure (<code>trestbps</code>) and exercise-induced angina
(<code>exang</code>) do not significantly predict heart
disease(<code>num</code>), and the relationship does not vary across
regions.</p>
<p>Alternative Hypothesis (<span class="math inline">\(H_a\)</span>): At
least one of these predictors significantly impacts heart disease, and
the relationship differs by region.</p>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{trestbps} + \beta_2 \cdot \text{exang} + \beta_3 \cdot
\text{region} + \beta_4 \cdot \text{(trestbps*region)} + \beta_5 \cdot
\text{(exang*region)} + \epsilon \]</span></p>
<pre class="r"><code># fit the model with interaction terms
region_model &lt;- lm(num ~ trestbps * region_factor + exang * region_factor, data = region_data)

# summarize the model
region_model_summary &lt;- broom::tidy(region_model)
region_model_summary</code></pre>
<pre><code>## # A tibble: 12 × 5
##    term                      estimate std.error statistic  p.value
##    &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 (Intercept)             -0.149       0.180     -0.828  4.08e- 1
##  2 trestbps                 0.00350     0.00136    2.58   1.03e- 2
##  3 region_factor2           0.165       0.365      0.451  6.52e- 1
##  4 region_factor3           0.927       0.324      2.86   4.39e- 3
##  5 region_factor4           1.01        0.460      2.20   2.86e- 2
##  6 exang                    0.450       0.0509     8.85   1.37e-17
##  7 trestbps:region_factor2  0.0000304   0.00270    0.0113 9.91e- 1
##  8 trestbps:region_factor3 -0.00481     0.00249   -1.93   5.45e- 2
##  9 trestbps:region_factor4 -0.00289     0.00343   -0.842  4.00e- 1
## 10 region_factor2:exang    -0.159       0.111     -1.43   1.53e- 1
## 11 region_factor3:exang    -0.154       0.121     -1.28   2.02e- 1
## 12 region_factor4:exang    -0.397       0.139     -2.85   4.49e- 3</code></pre>
<pre class="r"><code># Test significance of interaction terms
anova(region_model)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: num
##                         Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## trestbps                 1  3.030  3.0302 17.6341 3.151e-05 ***
## region_factor            3 17.205  5.7351 33.3748 &lt; 2.2e-16 ***
## exang                    1 14.190 14.1902 82.5789 &lt; 2.2e-16 ***
## trestbps:region_factor   3  1.053  0.3511  2.0430   0.10689    
## region_factor:exang      3  1.642  0.5474  3.1858   0.02355 *  
## Residuals              519 89.184  0.1718                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From the code result, we see that - <code>trestbps</code> is positive
(<span class="math inline">\(\beta_1 = 0.0035\)</span>) and significant
with p = 0.01027, indicating higher resting blood pressure modestly
increases the likelihood of heart disease. - <code>exang</code> is
positive (<span class="math inline">\(\beta_2 = 0.4504\)</span>) and
highly significant with p &lt; 2*10^-16, suggesting that individuals
with exercise-induced angina are more likely to have heart disease. -
<code>region_factor</code> shows significant differences in the baseline
likelihood of heart disease across regions: Baseline risk is higher in
<code>Region 3 - Long Beach</code> (<span class="math inline">\(\beta =
0.9268\)</span>, p = 0.00439) and <code>Region 4 - Switzerland</code>
(<span class="math inline">\(\beta = 1.010\)</span>, p = 0.02857)
compared to Cleveland. Significant interaction for
<code>exang * region</code> (p = 0.02355), Lower impact of
exercise-induced angina in Switzerland (<span
class="math inline">\(\beta = -0.3967\)</span>, p = 0.00449).
Interaction for <code>trestbps * region</code> is marginal (p=0.10689),
with some variability in Region 3 (Long Beach, p=0.05446).</p>
<ol start="2" style="list-style-type: decimal">
<li>Examining Predictive Power of Clinical Indicators for Heart Disease
in Diverse Populations Investigate how gender and age group modify the
relationships between key predictors and heart disease.</li>
</ol>
</div>
<div id="checking-datasets-1" class="section level2">
<h2>checking datasets</h2>
<pre class="r"><code># Refine age groups
age_group_data &lt;- combined_data_two %&gt;%
  mutate(age_group = case_when(
    age &lt; 40 ~ &quot;Under 40&quot;,
    age &gt;= 40 &amp; age &lt;= 60 ~ &quot;40-60&quot;,
    age &gt; 60 ~ &quot;Above 60&quot;
  ))

ggplot(age_group_data, aes(x = thalach, y = num, color = as.factor(sex))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(group = sex)) +
  labs(
    title = &quot;Heart Disease Status (`num`) vs Maximum Heart Rate (`thalach`) by Gender&quot;,
    x = &quot;Maximum Heart Rate (thalach)&quot;,
    y = &quot;Heart Disease Status (num)&quot;,
    color = &quot;Gender&quot;
  ) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>ggplot(age_group_data, aes(x = slope, y = num, color = as.factor(sex))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(group = sex)) +
  labs(
    title = &quot;Heart Disease Status (`num`) vs Slope of ST Segment (`slope`) by Gender&quot;,
    x = &quot;Slope of ST Segment (slope)&quot;,
    y = &quot;Heart Disease Status (num)&quot;,
    color = &quot;Gender&quot;
  ) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<pre class="r"><code>ggplot(age_group_data, aes(x = num, fill = as.factor(sex))) +
  geom_density(alpha = 0.5) +
  labs(
    title = &quot;Distribution of Heart Disease Status (`num`) by Gender&quot;,
    x = &quot;Heart Disease Status (num)&quot;,
    fill = &quot;Gender&quot;
  ) +
  theme_minimal()</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-9-3.png" width="672" /></p>
<p>A negative trend between thalach and heart disease status is visible
for both genders, with a steeper decline for males. This indicates that
lower maximum heart rate is more predictive of heart disease in males.
The relationship between slope and heart disease shows distinct patterns
across genders. Males have a stronger positive association between
higher slope values and the presence of heart disease. Heart disease
status distribution (num) differs between genders, with males (gender=1)
showing a higher density near num=1 (presence of heart disease). This
aligns with previous findings of gender disparities in heart disease
prevalence.</p>
<pre class="r"><code>ggplot(age_group_data, aes(x = thalach, y = num, color = age_group)) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(group = age_group)) +
  geom_point(alpha = 0.5) +
  labs(
    title = &quot;Heart Disease Status (`num`) vs Maximum Heart Rate (`thalach`) by Age Group&quot;,
    x = &quot;Maximum Heart Rate (thalach)&quot;,
    y = &quot;Heart Disease Status (num)&quot;,
    color = &quot;Age Group&quot;
  ) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>ggplot(age_group_data, aes(x = slope, y = num, color = age_group)) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(group = age_group)) +
  geom_point(alpha = 0.5) +
  labs(
    title = &quot;Heart Disease Status (`num`) vs Slope of ST Segment (`slope`) by Age Group&quot;,
    x = &quot;Slope of ST Segment (slope)&quot;,
    y = &quot;Heart Disease Status (num)&quot;,
    color = &quot;Age Group&quot;
  ) +
  theme_minimal()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<pre class="r"><code>ggplot(age_group_data, aes(x = num, fill = age_group)) +
  geom_density(alpha = 0.5) +
  labs(
    title = &quot;Distribution of Heart Disease Status (`num`) by Age Group&quot;,
    x = &quot;Heart Disease Status (num)&quot;,
    fill = &quot;Age Group&quot;
  ) +
  theme_minimal()</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-10-3.png" width="672" />
A negative association is evident across all age groups, but the trend
is most pronounced for the 40-60 age group. For individuals under 40 and
above 60, the association is weaker, potentially due to smaller sample
sizes or varying risk factors. Similar trends are observed across age
groups, with higher slope values generally predicting heart disease. The
relationship is strongest in the 40-60 age group, which also has the
largest sample size. The distribution of heart disease status differs by
age group. Individuals aged 40-60 have the highest density near num=1
(presence of heart disease), reflecting their higher overall risk in
this dataset.</p>
<p>##Hypothesis 1 (MLR): - <strong>Null Hypothesis (H_0):</strong> The
relationships between maximum heart rate (<code>thalach</code>), the
slope of the ST segment (<code>slope</code>), and heart disease status
(<code>num</code>) do not vary by gender - <strong>Alternative
Hypothesis (H_a):</strong> At least one of these predictors interacts
with gender to significantly influence heart disease status.</p>
<p><span class="math display">\[ num = \beta_0 + \beta_1 \cdot
\text{thalach} + \beta_2 \cdot \text{fbs} + \beta_3 \cdot \text{slope} +
\beta_4 \cdot \text{sex} + \beta_5 \cdot \text{thalach*sex} +
\beta_6  \cdot  \text{slope*sex} + \epsilon \]</span>
<code>thalach</code> and <code>slope</code> were identified as strong
predictors in logistic regression and EDA. <code>fbs</code> has weaker
significance and may be excluded in a stepwise selection process.</p>
<p>##Hypothesis 2 (MLR): - <strong>Null Hypothesis (H_0):</strong> The
relationships between maximum heart rate (<code>thalach</code>), the
slope of the ST segment (<code>slope</code>), and heart disease status
(<code>num</code>) do not vary by age group - <strong>Alternative
Hypothesis (H_a):</strong> At least one of these predictors interacts
with age group to significantly influence heart disease status. <span
class="math display">\[ num = \beta_0 + \beta_1 \cdot \text{thalach} +
\beta_2 \cdot \text{fbs} + \beta_3 \cdot \text{slope} + \beta_4 \cdot
\text{age_group} + \beta_5 \cdot \text{thalach*age_group} + \beta_6
\cdot \text{slope*age_group} + \epsilon \]</span></p>
<pre class="r"><code># Separate the data by gender
male_data &lt;- combined_data_two %&gt;% filter(sex == 1)
female_data &lt;- combined_data_two %&gt;% filter(sex == 0)

# Fit the model with interaction terms for gender
gender_model &lt;- lm(num ~ thalach * sex + slope * sex + fbs, data = combined_data_two)

# Summarize the model results
gender_model_summary &lt;- broom::tidy(gender_model)
gender_model_summary</code></pre>
<pre><code>## # A tibble: 7 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)  0.165     0.310       0.532 0.595     
## 2 thalach     -0.00261   0.00174    -1.50  0.133     
## 3 sex          1.22      0.346       3.54  0.000436  
## 4 slope        0.328     0.0677      4.84  0.00000169
## 5 fbs          0.0226    0.0487      0.463 0.644     
## 6 thalach:sex -0.00421   0.00193    -2.19  0.0292    
## 7 sex:slope   -0.196     0.0771     -2.55  0.0111</code></pre>
<pre class="r"><code># Test the significance of interaction terms
anova(gender_model)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: num
##              Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
## thalach       1 24.756 24.7555 150.8110 &lt; 2.2e-16 ***
## sex           1  9.080  9.0804  55.3179 4.218e-13 ***
## slope         1  5.066  5.0658  30.8608 4.416e-08 ***
## fbs           1  0.002  0.0015   0.0094   0.92279    
## thalach:sex   1  0.322  0.3216   1.9592   0.16219    
## sex:slope     1  1.066  1.0659   6.4934   0.01111 *  
## Residuals   524 86.014  0.1641                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Model Results: Significant Variables: Sex (p &lt; 0.001): Being male
significantly increases the likelihood of heart disease, with males
having higher odds of heart disease compared to females. Slope (p &lt;
0.001): A higher slope value (indicating abnormal ST depression) is
associated with an increased likelihood of heart disease. thalach:sex (p
= 0.029): The interaction between heart rate and gender is significant.
This suggests that the relationship between heart rate and heart disease
differs between males and females. Specifically, males may show a
stronger association with heart rate. sex:slope (p = 0.011): The
relationship between the slope of the ST segment and heart disease
differs significantly between males and females, with a stronger
association in males.</p>
<p>The interaction terms <code>thalach:sex</code> and
<code>sex:slope</code> are significant, indicating that the
relationships between thalach and heart disease, as well as slope and
heart disease, differ by gender. Hence, gender modifies the relationship
between clinical indicators and heart disease, supporting the
alternative hypothesis.</p>
<pre class="r"><code># Summary of the dataset by age group
table(age_group_data$age_group)</code></pre>
<pre><code>## 
##    40-60 Above 60 Under 40 
##      362      143       26</code></pre>
<pre class="r"><code># Fit the model with interaction terms for age group
age_group_model &lt;- lm(num ~ thalach * age_group + slope * age_group + fbs, data = age_group_data)

# Summarize the model results
age_group_summary &lt;- broom::tidy(age_group_model)
age_group_summary</code></pre>
<pre><code>## # A tibble: 10 × 5
##    term                      estimate std.error statistic  p.value
##    &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 (Intercept)                1.17     0.176        6.67  6.66e-11
##  2 thalach                   -0.00679  0.000945    -7.19  2.29e-12
##  3 age_groupAbove 60         -0.156    0.320       -0.486 6.27e- 1
##  4 age_groupUnder 40          1.47     0.910        1.62  1.06e- 1
##  5 slope                      0.210    0.0420       4.99  8.35e- 7
##  6 fbs                        0.0285   0.0511       0.557 5.78e- 1
##  7 thalach:age_groupAbove 60  0.00203  0.00188      1.08  2.82e- 1
##  8 thalach:age_groupUnder 40 -0.00761  0.00446     -1.71  8.83e- 2
##  9 age_groupAbove 60:slope   -0.0517   0.0722      -0.716 4.74e- 1
## 10 age_groupUnder 40:slope   -0.124    0.170       -0.728 4.67e- 1</code></pre>
<pre class="r"><code># Test the significance of interaction terms
anova(age_group_model)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: num
##                    Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
## thalach             1 24.756 24.7555 136.9416 &lt; 2.2e-16 ***
## age_group           2  0.151  0.0756   0.4182   0.65846    
## slope               1  6.101  6.1012  33.7505 1.091e-08 ***
## fbs                 1  0.054  0.0544   0.3010   0.58347    
## thalach:age_group   2  0.894  0.4471   2.4732   0.08531 .  
## age_group:slope     2  0.165  0.0825   0.4565   0.63377    
## Residuals         521 94.183  0.1808                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Model Results: Significant Variables: thalach (p &lt; 0.001): Lower
maximum heart rates are consistently associated with a higher likelihood
of heart disease, regardless of age group. Slope (p &lt; 0.001): A
steeper ST segment slope is associated with a higher likelihood of heart
disease, with a stronger association across age groups. Age Group (Above
60) (p = 0.203): The effect of being above 60 years old is not
significant, suggesting that this group does not have a significantly
different relationship with heart disease compared to the 40-60 group.
Age Group (Under 40) (p = 0.103): Similar to the above-60 group, the
under-40 age group shows a marginally non-significant relationship with
heart disease.</p>
<p>Interaction Effects: thalach:age_groupAbove 60 (p = 0.168): This
interaction is non-significant, indicating that the effect of heart rate
on heart disease does not vary significantly for individuals over 60.
thalach:age_groupUnder 40 (p = 0.113): The interaction between heart
rate and being under 40 is marginally non-significant, suggesting only a
weak difference in how heart rate affects heart disease for younger
individuals.</p>
<p>Therefore, age does not modify the relationship between heart rate,
slope, and heart disease in a statistically significant way. This does
not support the alternative hypothesis, and the null hypothesis cannot
be rejected.</p>
<pre class="r"><code># Stepwise selection for the gender interaction model
stepwise_gender_model &lt;- step(gender_model, direction = &quot;both&quot;)</code></pre>
<pre><code>## Start:  AIC=-952.55
## num ~ thalach * sex + slope * sex + fbs
## 
##               Df Sum of Sq    RSS     AIC
## - fbs          1   0.03520 86.050 -954.33
## &lt;none&gt;                     86.014 -952.55
## - thalach:sex  1   0.78456 86.799 -949.73
## - sex:slope    1   1.06589 87.080 -948.01
## 
## Step:  AIC=-954.33
## num ~ thalach + sex + slope + thalach:sex + sex:slope
## 
##               Df Sum of Sq    RSS     AIC
## &lt;none&gt;                     86.050 -954.33
## + fbs          1   0.03520 86.014 -952.55
## - thalach:sex  1   0.75965 86.809 -951.67
## - sex:slope    1   1.03649 87.086 -949.98</code></pre>
<pre class="r"><code># Summarize the stepwise model results
stepwise_gender_summary &lt;- broom::tidy(stepwise_gender_model)
stepwise_gender_summary</code></pre>
<pre><code>## # A tibble: 6 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)  0.178     0.309       0.576 0.565     
## 2 thalach     -0.00266   0.00173    -1.54  0.124     
## 3 sex          1.21      0.343       3.51  0.000482  
## 4 slope        0.326     0.0676      4.83  0.00000180
## 5 thalach:sex -0.00413   0.00192    -2.15  0.0318    
## 6 sex:slope   -0.193     0.0766     -2.51  0.0122</code></pre>
<pre class="r"><code># Stepwise selection for the age group interaction model
stepwise_age_group_model &lt;- step(age_group_model, direction = &quot;both&quot;)</code></pre>
<pre><code>## Start:  AIC=-898.37
## num ~ thalach * age_group + slope * age_group + fbs
## 
##                     Df Sum of Sq    RSS     AIC
## - age_group:slope    2   0.16503 94.349 -901.44
## - fbs                1   0.05602 94.240 -900.06
## &lt;none&gt;                           94.183 -898.37
## - thalach:age_group  2   0.81660 95.000 -897.79
## 
## Step:  AIC=-901.44
## num ~ thalach + age_group + slope + fbs + thalach:age_group
## 
##                     Df Sum of Sq    RSS     AIC
## - fbs                1    0.0519 94.400 -903.15
## &lt;none&gt;                           94.349 -901.44
## - thalach:age_group  2    0.8942 95.243 -900.44
## + age_group:slope    2    0.1650 94.183 -898.37
## - slope              1    5.6478 99.996 -872.57
## 
## Step:  AIC=-903.15
## num ~ thalach + age_group + slope + thalach:age_group
## 
##                     Df Sum of Sq     RSS     AIC
## &lt;none&gt;                            94.400 -903.15
## - thalach:age_group  2    0.8966  95.297 -902.13
## + fbs                1    0.0519  94.349 -901.44
## + age_group:slope    2    0.1610  94.240 -900.06
## - slope              1    5.8231 100.224 -873.37</code></pre>
<pre class="r"><code># Summarize the stepwise model results
stepwise_age_group_summary &lt;- broom::tidy(stepwise_age_group_model)
stepwise_age_group_summary</code></pre>
<pre><code>## # A tibble: 7 × 5
##   term                      estimate std.error statistic  p.value
##   &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)                1.23     0.158         7.79 3.52e-14
## 2 thalach                   -0.00696  0.000914     -7.61 1.26e-13
## 3 age_groupAbove 60         -0.306    0.240        -1.27 2.03e- 1
## 4 age_groupUnder 40          0.989    0.605         1.63 1.03e- 1
## 5 slope                      0.190    0.0334        5.69 2.17e- 8
## 6 thalach:age_groupAbove 60  0.00247  0.00179       1.38 1.68e- 1
## 7 thalach:age_groupUnder 40 -0.00593  0.00373      -1.59 1.13e- 1</code></pre>
<p>For the gender interaction model: The stepwise selection process
retained thalach, sex, slope, and their interactions
(<code>thalach:sex</code> and <code>sex:slope</code>) as significant
predictors. The model explains 31.87% of the variance in heart disease
status, with a highly significant F-statistic. This suggests that the
significant interactions between gender and clinical indicators (heart
rate and slope) are important predictors for heart disease. So the final
model should be <span class="math display">\[ num = \beta_0 + \beta_1
\cdot \text{thalach} + \beta_2 \cdot \text{slope} + \beta_3 \cdot
\text{sex} + \beta_4 \cdot \text{thalach*sex} + \beta_5  \cdot
\text{slope*sex} + \epsilon \]</span></p>
<p>For the age group interaction model: After stepwise selection, the
final model includes thalach, age_group, slope, and
<code>thalach:age_group</code>. The model explains 25.26% of the
variance, with significant contributions from thalach and slope.
However, the interaction terms (thalach:age_group) are not significant,
confirming that age group does not substantially modify the
relationships between heart disease and these clinical indicators. So
the final model should be <span class="math display">\[ num = \beta_0 +
\beta_1 \cdot \text{thalach} + \beta_2 \cdot \text{slope} + \beta_3
\cdot \text{age_group} + \beta_4 \cdot \text{thalach*age_group}+
\epsilon \]</span> Given that num (heart disease status) is binary,
future analyses should consider logistic regression for more accurate
risk prediction based on diagnostic factors across regions/
demographics.</p>
</div>
</div>
<div id="logistic-regression-analyses" class="section level1">
<h1>Logistic Regression Analyses</h1>
<pre class="r"><code>cleveland &lt;- read.csv(&quot;./data/cleveland.csv&quot;, header = FALSE)
hungarian &lt;- read.csv(&quot;./data/hungarian.csv&quot;, header = FALSE)
long_beach &lt;- read.csv(&quot;./data/long_beach_va.csv&quot;, header = FALSE)
switzerland &lt;- read.csv(&quot;./data/switzerland.csv&quot;, header = FALSE)

# Add region column and combine datasets
cleveland$region &lt;- &quot;Cleveland&quot;
hungarian$region &lt;- &quot;Hungarian&quot;
long_beach$region &lt;- &quot;Long_Beach_VA&quot;
switzerland$region &lt;- &quot;Switzerland&quot;
combined_data &lt;- bind_rows(cleveland, hungarian, long_beach, switzerland)

colnames(combined_data) &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;fbs&quot;,
                             &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, &quot;slope&quot;,
                             &quot;ca&quot;, &quot;thal&quot;, &quot;num&quot;, &quot;region&quot;)
combined_data &lt;- combined_data %&gt;%
  mutate(across(c(age, sex, cp, trestbps, chol, fbs, restecg, thalach,
                  exang, oldpeak, slope, ca, thal, num), as.numeric))</code></pre>
<pre><code>## Warning: There were 14 warnings in `mutate()`.
## The first warning was:
## ℹ In argument: `across(...)`.
## Caused by warning:
## ! 强制改变过程中产生了NA
## ℹ Run `dplyr::last_dplyr_warnings()` to see the 13 remaining warnings.</code></pre>
<p>Dropped variables with excessive missing values (ca and thal).</p>
<p>Removed rows with missing values in critical variables.</p>
<p>Converted num to binary (0 = no heart disease, 1 = heart disease) and
set as a factor.</p>
<pre class="r"><code># Clean the data
critical_columns &lt;- c(&quot;num&quot;, &quot;age&quot;, &quot;sex&quot;, &quot;cp&quot;, &quot;trestbps&quot;, &quot;chol&quot;,
                      &quot;fbs&quot;, &quot;restecg&quot;, &quot;thalach&quot;, &quot;exang&quot;, &quot;oldpeak&quot;, 
                      &quot;slope&quot;, &quot;region&quot;)

cleaned_data &lt;- combined_data %&gt;% 
  select(all_of(critical_columns)) %&gt;% 
  drop_na()  # Drop rows with NA values in the critical columns

# Recode &#39;num&#39; as a binary factor
cleaned_data &lt;- cleaned_data %&gt;%
  mutate(num = as.factor(ifelse(num &gt; 0, 1, 0)))

logistic_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                      restecg + thalach + exang + oldpeak + region,
                      data = cleaned_data, family = binomial)

logistic_summary &lt;- broom::tidy(logistic_model)
logistic_summary</code></pre>
<pre><code>## # A tibble: 14 × 5
##    term                estimate std.error statistic     p.value
##    &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
##  1 (Intercept)         -4.53      1.81       -2.51  0.0122     
##  2 age                  0.0162    0.0161      1.00  0.315      
##  3 sex                  1.47      0.292       5.03  0.000000487
##  4 cp                   0.668     0.139       4.81  0.00000152 
##  5 trestbps             0.00987   0.00711     1.39  0.165      
##  6 chol                 0.00110   0.00182     0.607 0.544      
##  7 fbs                  0.0897    0.342       0.262 0.793      
##  8 restecg              0.167     0.142       1.18  0.239      
##  9 thalach             -0.0161    0.00637    -2.52  0.0116     
## 10 exang                0.900     0.279       3.22  0.00129    
## 11 oldpeak              0.696     0.136       5.11  0.000000329
## 12 regionHungarian      0.179     0.387       0.463 0.644      
## 13 regionLong_Beach_VA  0.121     0.436       0.277 0.782      
## 14 regionSwitzerland    3.84      1.16        3.31  0.000945</code></pre>
<p>Significant Predictors (p-value &lt; 0.05):</p>
<p>sex (1.409350): <em>Being male increases the log-odds of heart
disease significantly.</em></p>
<p>cp (chest pain): <em>Higher chest pain levels increase the log-odds
of heart disease.</em></p>
<p>thalach (max heart rate achieved): <em>Higher heart rates decrease
the log-odds of heart disease.</em></p>
<p>exang (exercise-induced angina): <em>Presence of exercise-induced
angina increases the odds of heart disease.</em></p>
<p>oldpeak (ST depression): <em>Higher ST depression values
significantly increase the odds of heart disease.</em></p>
<p>regionSwitzerland: <em>Patients from Switzerland have significantly
higher odds of heart disease compared to the reference region
(Cleveland).</em></p>
<p>Non-Significant Predictors (p-value &gt; 0.05): age, trestbps
(resting blood pressure), chol (cholesterol), fbs (fasting blood sugar),
restecg (resting ECG results), regionHungarian, regionLong_Beach_VA.</p>
<p>Regional Effects: Patients from Switzerland have much higher odds of
heart disease compared to Cleveland, Hungarian and Long Beach VA.</p>
<div id="separated-by-region" class="section level3">
<h3>Separated by region:</h3>
<pre class="r"><code>extract_results &lt;- function(model, region) {
  if (inherits(model, &quot;try-error&quot;)) {
    return(data.frame(
      Region = region,
      Variable = NA,
      Estimate = NA,
      Std_Error = NA,
      P_Value = NA
    ))
  }
  
  coefficients &lt;- tryCatch({
    summary(model)$coefficients
  }, error = function(e) {
    return(NULL)
  })
  
  if (is.null(coefficients)) {
    return(data.frame(
      Region = region,
      Variable = NA,
      Estimate = NA,
      Std_Error = NA,
      P_Value = NA
    ))
  }
  
  data.frame(
    Region = region,
    Variable = rownames(coefficients),
    Estimate = coefficients[, &quot;Estimate&quot;],
    Std_Error = coefficients[, &quot;Std. Error&quot;],
    P_Value = coefficients[, &quot;Pr(&gt;|z|)&quot;]
  )
}

# Filter data for each region
cleveland_data &lt;- cleaned_data %&gt;% filter(region == &quot;Cleveland&quot;)
hungarian_data &lt;- cleaned_data %&gt;% filter(region == &quot;Hungarian&quot;)
long_beach_data &lt;- cleaned_data %&gt;% filter(region == &quot;Long_Beach_VA&quot;)
switzerland_data &lt;- cleaned_data %&gt;% filter(region == &quot;Switzerland&quot;)

# Fit logistic regression models
cleveland_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                       restecg + thalach + exang + oldpeak,
                       data = cleveland_data, family = binomial)
hungarian_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                       restecg + thalach + exang + oldpeak,
                       data = hungarian_data, family = binomial)
long_beach_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                        restecg + thalach + exang + oldpeak,
                        data = long_beach_data, family = binomial)
switzerland_model &lt;- glm(num ~ age + sex + cp + trestbps + chol + fbs +
                         restecg + thalach + exang + oldpeak,
                         data = switzerland_data, family = binomial)</code></pre>
<pre><code>## Warning: glm.fit:算法没有聚合</code></pre>
<pre><code>## Warning: glm.fit:拟合概率算出来是数值零或一</code></pre>
<pre class="r"><code># Extract results for each region
cleveland_results &lt;- extract_results(cleveland_model, &quot;Cleveland&quot;)
hungarian_results &lt;- extract_results(hungarian_model, &quot;Hungarian&quot;)
long_beach_results &lt;- extract_results(long_beach_model, &quot;Long_Beach_VA&quot;)
switzerland_results &lt;- extract_results(switzerland_model, &quot;Switzerland&quot;)

# Combine results
regional_results &lt;- bind_rows(cleveland_results, hungarian_results,
                               long_beach_results, switzerland_results)

# View the consolidated results
regional_results</code></pre>
<pre><code>##                         Region    Variable      Estimate    Std_Error
## (Intercept)...1      Cleveland (Intercept) -6.409349e+00 2.366230e+00
## age...2              Cleveland         age  2.305274e-02 2.078413e-02
## sex...3              Cleveland         sex  1.914565e+00 3.963041e-01
## cp...4               Cleveland          cp  8.005256e-01 1.784567e-01
## trestbps...5         Cleveland    trestbps  1.933797e-02 9.694877e-03
## chol...6             Cleveland        chol  5.199003e-03 3.249779e-03
## fbs...7              Cleveland         fbs -1.801118e-01 4.422953e-01
## restecg...8          Cleveland     restecg  2.186373e-01 1.622814e-01
## thalach...9          Cleveland     thalach -2.566683e-02 9.002007e-03
## exang...10           Cleveland       exang  1.016419e+00 3.608669e-01
## oldpeak...11         Cleveland     oldpeak  5.908489e-01 1.596412e-01
## (Intercept)...12     Hungarian (Intercept)  9.770581e-01 5.205882e+00
## age...13             Hungarian         age -8.168691e-02 5.314665e-02
## sex...14             Hungarian         sex  1.356069e+00 6.866089e-01
## cp...15              Hungarian          cp  7.316002e-01 3.775361e-01
## trestbps...16        Hungarian    trestbps  3.011956e-02 1.858612e-02
## chol...17            Hungarian        chol  8.921158e-04 5.398937e-03
## fbs...18             Hungarian         fbs  1.801468e+01 1.879517e+03
## restecg...19         Hungarian     restecg -6.322004e-01 7.733075e-01
## thalach...20         Hungarian     thalach -3.876461e-02 1.929788e-02
## exang...21           Hungarian       exang  1.502774e-01 7.325074e-01
## oldpeak...22         Hungarian     oldpeak  9.291678e-01 5.364974e-01
## (Intercept)...23 Long_Beach_VA (Intercept) -1.366177e-01 4.132572e+00
## age...24         Long_Beach_VA         age  1.796714e-02 4.359182e-02
## sex...25         Long_Beach_VA         sex  4.559604e-01 1.494038e+00
## cp...26          Long_Beach_VA          cp -2.887811e-02 4.302410e-01
## trestbps...27    Long_Beach_VA    trestbps -1.079782e-02 1.676406e-02
## chol...28        Long_Beach_VA        chol -7.260783e-04 2.764071e-03
## fbs...29         Long_Beach_VA         fbs  1.762377e-01 7.424995e-01
## restecg...30     Long_Beach_VA     restecg -3.073046e-01 4.673256e-01
## thalach...31     Long_Beach_VA     thalach  1.472768e-03 1.510586e-02
## exang...32       Long_Beach_VA       exang  1.774583e+00 8.143903e-01
## oldpeak...33     Long_Beach_VA     oldpeak  4.119425e-01 3.761805e-01
## (Intercept)...34   Switzerland (Intercept) -4.864726e+02 3.401466e+05
## age...35           Switzerland         age -1.761182e+01 6.992787e+03
## sex...36           Switzerland         sex  1.400754e+02 1.953051e+05
## cp...37            Switzerland          cp  2.696132e+00 1.203799e+04
## trestbps...38      Switzerland    trestbps  6.806273e+00 2.736445e+03
## fbs...39           Switzerland         fbs -8.133709e+01 1.467447e+05
## restecg...40       Switzerland     restecg  9.072661e+01 1.413873e+05
## thalach...41       Switzerland     thalach  5.029885e+00 1.969977e+03
## exang...42         Switzerland       exang  2.544557e+02 1.604791e+05
## oldpeak...43       Switzerland     oldpeak -1.201417e+02 8.414472e+04
##                       P_Value
## (Intercept)...1  6.755232e-03
## age...2          2.673649e-01
## sex...3          1.358144e-06
## cp...4           7.263202e-06
## trestbps...5     4.608011e-02
## chol...6         1.096425e-01
## fbs...7          6.838460e-01
## restecg...8      1.778924e-01
## thalach...9      4.354982e-03
## exang...10       4.853456e-03
## oldpeak...11     2.146612e-04
## (Intercept)...12 8.511248e-01
## age...13         1.242909e-01
## sex...14         4.826551e-02
## cp...15          5.264416e-02
## trestbps...16    1.051161e-01
## chol...17        8.687557e-01
## fbs...18         9.923526e-01
## restecg...19     4.136269e-01
## thalach...20     4.456365e-02
## exang...21       8.374512e-01
## oldpeak...22     8.328873e-02
## (Intercept)...23 9.736277e-01
## age...24         6.802165e-01
## sex...25         7.602240e-01
## cp...26          9.464856e-01
## trestbps...27    5.195072e-01
## chol...28        7.927939e-01
## fbs...29         8.123796e-01
## restecg...30     5.108072e-01
## thalach...31     9.223321e-01
## exang...32       2.932927e-02
## oldpeak...33     2.734877e-01
## (Intercept)...34 9.988589e-01
## age...35         9.979905e-01
## sex...36         9.994277e-01
## cp...37          9.998213e-01
## trestbps...38    9.980154e-01
## fbs...39         9.995578e-01
## restecg...40     9.994880e-01
## thalach...41     9.979628e-01
## exang...42       9.987349e-01
## oldpeak...43     9.988608e-01</code></pre>
<p>The logistic regression analysis separated by region failed due to
insufficient sample sizes within each region. Logistic regression
requires an adequate number of observations to produce reliable
estimates. When the data was split by region, the limited number of
cases led to unstable coefficients, inflated errors, and convergence
issues. This highlights that the sample size per region was too small
for meaningful analysis.</p>
</div>
<div id="separated-by-gender" class="section level3">
<h3>Separated by gender:</h3>
<pre class="r"><code># Filter data for males and females
male_data &lt;- cleaned_data %&gt;% filter(sex == 1)
female_data &lt;- cleaned_data %&gt;% filter(sex == 0)

# Fit logistic regression models
male_model &lt;- glm(num ~ age + cp + trestbps + chol + fbs +
                  restecg + thalach + exang + oldpeak,
                  data = male_data, family = binomial)
female_model &lt;- glm(num ~ age + cp + trestbps + chol + fbs +
                    restecg + thalach + exang + oldpeak,
                    data = female_data, family = binomial)

# Function to extract model results
extract_results &lt;- function(model, gender) {
  if (inherits(model, &quot;try-error&quot;)) {
    return(data.frame(
      Gender = gender,
      Variable = NA,
      Estimate = NA,
      Std_Error = NA,
      P_Value = NA
    ))
  }
  
  coefficients &lt;- tryCatch({
    summary(model)$coefficients
  }, error = function(e) {
    return(NULL)
  })
  
  if (is.null(coefficients)) {
    return(data.frame(
      Gender = gender,
      Variable = NA,
      Estimate = NA,
      Std_Error = NA,
      P_Value = NA
    ))
  }
  
  data.frame(
    Gender = gender,
    Variable = rownames(coefficients),
    Estimate = coefficients[, &quot;Estimate&quot;],
    Std_Error = coefficients[, &quot;Std. Error&quot;],
    P_Value = coefficients[, &quot;Pr(&gt;|z|)&quot;]
  )
}

# Extract results for males and females
male_results &lt;- extract_results(male_model, &quot;Male&quot;)
female_results &lt;- extract_results(female_model, &quot;Female&quot;)

# Combine results into a single data frame
gender_results &lt;- bind_rows(male_results, female_results)

# View the results
gender_results</code></pre>
<pre><code>##                  Gender    Variable     Estimate   Std_Error      P_Value
## (Intercept)...1    Male (Intercept)  0.489143833 1.883367077 7.950815e-01
## age...2            Male         age  0.011078627 0.017509693 5.269205e-01
## cp...3             Male          cp  0.624244440 0.150392680 3.313681e-05
## trestbps...4       Male    trestbps  0.006669625 0.007709166 3.869536e-01
## chol...5           Male        chol -0.002816539 0.001546731 6.861250e-02
## fbs...6            Male         fbs -0.077076106 0.364747670 8.326427e-01
## restecg...7        Male     restecg  0.211261443 0.158412456 1.823296e-01
## thalach...8        Male     thalach -0.026110546 0.006843324 1.359200e-04
## exang...9          Male       exang  0.717197611 0.319844029 2.493970e-02
## oldpeak...10       Male     oldpeak  0.568585698 0.143981103 7.846851e-05
## (Intercept)...11 Female (Intercept) -7.814925653 3.385006792 2.096088e-02
## age...12         Female         age  0.011405862 0.030248298 7.061182e-01
## cp...13          Female          cp  0.927630109 0.341356680 6.578104e-03
## trestbps...14    Female    trestbps  0.033262774 0.014713351 2.377657e-02
## chol...15        Female        chol -0.003238931 0.003523281 3.579418e-01
## fbs...16         Female         fbs  0.917958044 0.873286597 2.931882e-01
## restecg...17     Female     restecg  0.017694465 0.282086696 9.499839e-01
## thalach...18     Female     thalach -0.011055801 0.012020177 3.576929e-01
## exang...19       Female       exang  1.143549158 0.548980044 3.724728e-02
## oldpeak...20     Female     oldpeak  0.677208667 0.291294270 2.008123e-02</code></pre>
<p>Males Significant Predictors (p-value &lt; 0.05):</p>
<p>Chest Pain (cp): Higher chest pain levels increase the odds of heart
disease.</p>
<p>Max Heart Rate (thalach): Higher maximum heart rates reduce the odds
of heart disease.</p>
<p>Exercise-Induced Angina (exang): Presence of exercise-induced angina
significantly increases the odds of heart disease.</p>
<p>ST Depression (oldpeak): Higher ST depression significantly increases
the odds of heart disease.</p>
<p>Non-Significant Predictors: age, trestbps, chol, fbs, restecg.</p>
<p>Females Significant Predictors (p-value &lt; 0.05):</p>
<p>Chest Pain (cp) : Stronger effect than males.</p>
<p>Exercise-Induced Angina (exang) : Stronger effect than males.</p>
<p>ST Depression (oldpeak): Similar effect to males.</p>
<p>Non-Significant Predictors: age, trestbps, chol, fbs, restecg,
thalach.</p>
<p>Conclusions Predictors for Both Genders: cp, exang, and oldpeak are
significant predictors for both males and females.</p>
<p>Gender-Specific Differences: Stronger effects of cp and exang in
females suggest potential gender-specific diagnostic markers for heart
disease.</p>
</div>
<div id="try-random-forest-classifier" class="section level2">
<h2>Try Random Forest Classifier!</h2>
<div id="a-brief-intro-to-random-forest-algorithm"
class="section level3">
<h3>A brief intro to Random Forest Algorithm</h3>
<p>Random Forest is an ensemble learning algorithm used for
classification and regression tasks. It builds multiple decision trees
using bootstrap sampling (random subsets of data) and selects features
randomly at each split to increase diversity. Each tree predicts
independently, and the final output is determined by majority voting
(classification) or averaging (regression). Random Forest is robust to
overfitting, handles high-dimensional data well, and provides feature
importance scores.</p>
</div>
<div
id="first-construct-the-model-with-all-predictors-and-then-show-the-feature-importance-trends-the-trend-is-descending-according-to-the-meandecreaseaccuracy"
class="section level3">
<h3>First, construct the model with all predictors and then show the
feature importance trends (the trend is descending according to the
MeanDecreaseAccuracy)!</h3>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Warning: 程序包&#39;caret&#39;是用R版本4.4.2 来建造的</code></pre>
<pre><code>## 载入需要的程序包：lattice</code></pre>
<pre><code>## 
## 载入程序包：&#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<pre class="r"><code>library(randomForest)</code></pre>
<pre><code>## Warning: 程序包&#39;randomForest&#39;是用R版本4.4.2 来建造的</code></pre>
<pre><code>## randomForest 4.7-1.2</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## 载入程序包：&#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre class="r"><code># drop out the variable &quot;ca&quot; and &quot;thal&quot; which are have so many missing values inside
variables = c(&quot;cp&quot;, &quot;age&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;, &quot;num&quot;, &quot;restecg&quot;, &quot;fbs&quot;,&quot;trestbps&quot;,&quot;region&quot;, &quot;slope&quot;, &quot;exang&quot;)
data = combined_data_two[, variables]        
data$num = as.factor(data$num)

# check and deal with missing data
if (any(is.na(data))) {
  print(&quot;Missing value detected&quot;)
  data = na.omit(data)
  print(&quot;Missing data have been deleted&quot;)
}
# split the dataset into training and testing datasets
set.seed(42)
trainIndex = createDataPartition(data$num, p = 0.8, list = FALSE)
trainData = data[trainIndex, ]
testData = data[-trainIndex, ]

# Construct the random forest model
rf_model = randomForest(num ~ ., data = trainData, importance = TRUE)
rf_pred = predict(rf_model, testData)
rf_conf_matrix = confusionMatrix(rf_pred, testData$num)

# Then we show the feature importance trends(The trend is descending according to the MeanDecreaseAccuracy)
var_imp = importance(rf_model)
var_imp_df = as.data.frame(var_imp)
var_imp_df$Variable = rownames(var_imp_df)
rownames(var_imp_df) = NULL
var_imp_df = var_imp_df[order(var_imp_df$MeanDecreaseAccuracy, decreasing = TRUE), ]
ggplot(var_imp_df, aes(x = reorder(Variable, -MeanDecreaseAccuracy))) +
  geom_line(aes(y = MeanDecreaseAccuracy, group = 1, color = &quot;MeanDecreaseAccuracy&quot;)) +
  geom_point(aes(y = MeanDecreaseAccuracy, color = &quot;MeanDecreaseAccuracy&quot;)) +
  geom_line(aes(y = MeanDecreaseGini, group = 1, color = &quot;MeanDecreaseGini&quot;)) +
  geom_point(aes(y = MeanDecreaseGini, color = &quot;MeanDecreaseGini&quot;)) +
  labs(title = &quot;Feature Importance Trends&quot;,
       x = &quot;Features&quot;,
       y = &quot;Importance&quot;,
       color = &quot;Metric&quot;) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1))</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-18-1.png" width="960" /></p>
<p>Then we ranked the predictors descendingly based on the
MeanDecreaseAccuracy which measures the decrease in overall model
accuracy when the variable is permuted. And we show them in a line plot!
Based on the MeanDecreaseAccuracy and MeanDecreaseGini, we can drop out
restecg and fbs predictors that have relatively small impact on our
prediction. And then we can focus on the first seven predictors that
have more impact on our prediction results!</p>
</div>
<div
id="then-drop-unsignificant-features-and-build-the-model-again-show-the-confusion-matrix"
class="section level3">
<h3>Then, drop unsignificant features and build the model again, show
the confusion matrix</h3>
<pre class="r"><code>variables2 = c(&quot;cp&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;, &quot;num&quot;,&quot;trestbps&quot;,&quot;region&quot;,&quot;slope&quot;, &quot;exang&quot;,&quot;age&quot;)
data2 = combined_data_two[, variables2]
data2$num = as.factor(data2$num)

set.seed(50)

trainIndex2 = createDataPartition(data2$num, p = 0.8, list = FALSE)
trainData2 = data2[trainIndex2, ]
testData2 = data2[-trainIndex2, ]

# Construct the random forest model and evaluate the model results
rf_model2 = randomForest(num ~ ., data = trainData2, importance = TRUE)
rf_pred2 = predict(rf_model2, testData2)
rf_conf_matrix2 = confusionMatrix(rf_pred2, testData2$num)

cm = rf_conf_matrix2$table
cm_df = as.data.frame(cm)
colnames(cm_df) = c(&quot;Predicted&quot;, &quot;Actual&quot;, &quot;Frequency&quot;)
cm_df$Proportion = cm_df$Frequency / sum(cm_df$Frequency)

ggplot(data = cm_df, aes(x = Predicted, y = Actual, fill = Proportion)) +
  geom_tile(color = &quot;white&quot;) +
  scale_fill_gradient(low = &quot;white&quot;, high = &quot;steelblue&quot;, name = &quot;Proportion&quot;) +
  geom_text(aes(label = Frequency), color = &quot;black&quot;, size = 5) +  # Add frequency labels
  labs(
    title = &quot;Confusion Matrix&quot;,
    x = &quot;Predicted Class&quot;,
    y = &quot;Actual Class&quot;
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = &quot;bold&quot;, hjust = 0.5),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="extracting-detailed-statistics-from-the-confusion-matrix"
class="section level3">
<h3>Extracting detailed statistics from the confusion matrix</h3>
<pre class="r"><code>stats = rf_conf_matrix2$overall
class_stats = rf_conf_matrix2$byClass

# Displaying overall statistics
cat(&quot;Overall Statistics:\n&quot;)</code></pre>
<pre><code>## Overall Statistics:</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Accuracy: %.4f\n&quot;, stats[&quot;Accuracy&quot;]))</code></pre>
<pre><code>## Accuracy: 0.8095</code></pre>
<pre class="r"><code>cat(sprintf(&quot;95%% CI: (%.4f, %.4f)\n&quot;, stats[&quot;AccuracyLower&quot;], stats[&quot;AccuracyUpper&quot;]))</code></pre>
<pre><code>## 95% CI: (0.7213, 0.8796)</code></pre>
<pre class="r"><code>cat(sprintf(&quot;No Information Rate: %.4f\n&quot;, stats[&quot;AccuracyNull&quot;]))</code></pre>
<pre><code>## No Information Rate: 0.6095</code></pre>
<pre class="r"><code>cat(sprintf(&quot;P-Value [Acc &gt; NIR]: %.6f\n&quot;, stats[&quot;AccuracyPValue&quot;]))</code></pre>
<pre><code>## P-Value [Acc &gt; NIR]: 0.000009</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Kappa: %.4f\n&quot;, stats[&quot;Kappa&quot;]))</code></pre>
<pre><code>## Kappa: 0.6067</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Mcnemar&#39;s Test P-Value: %.4f\n\n&quot;, stats[&quot;McnemarPValue&quot;]))</code></pre>
<pre><code>## Mcnemar&#39;s Test P-Value: 0.5023</code></pre>
<pre class="r"><code># Displaying class-specific statistics
cat(&quot;Class-Specific Statistics:\n&quot;)</code></pre>
<pre><code>## Class-Specific Statistics:</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Sensitivity: %.4f\n&quot;, class_stats[&quot;Sensitivity&quot;]))</code></pre>
<pre><code>## Sensitivity: 0.8049</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Specificity: %.4f\n&quot;, class_stats[&quot;Specificity&quot;]))</code></pre>
<pre><code>## Specificity: 0.8125</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Pos Pred Value: %.4f\n&quot;, class_stats[&quot;Pos Pred Value&quot;]))</code></pre>
<pre><code>## Pos Pred Value: 0.7333</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Neg Pred Value: %.4f\n&quot;, class_stats[&quot;Neg Pred Value&quot;]))</code></pre>
<pre><code>## Neg Pred Value: 0.8667</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Prevalence: %.4f\n&quot;, class_stats[&quot;Prevalence&quot;]))</code></pre>
<pre><code>## Prevalence: 0.3905</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Detection Rate: %.4f\n&quot;, class_stats[&quot;Detection Rate&quot;]))</code></pre>
<pre><code>## Detection Rate: 0.3143</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Detection Prevalence: %.4f\n&quot;, class_stats[&quot;Detection Prevalence&quot;]))</code></pre>
<pre><code>## Detection Prevalence: 0.4286</code></pre>
<pre class="r"><code>cat(sprintf(&quot;Balanced Accuracy: %.4f\n&quot;, class_stats[&quot;Balanced Accuracy&quot;]))</code></pre>
<pre><code>## Balanced Accuracy: 0.8087</code></pre>
<p>From the model we can observe the following things: 1. The model
correctly classified 80.95% of the instances. 2. The true accuracy is
expected to fall 95% of the time in (0.7213, 0.8796) 3. The no
information rate is 0.6095 which is less than the accuracy rate (p-value
also indicate this), indicating the model we built actually capture some
significant features. 4. The Kappa is 0.6067 which is in the range
(0.6,0.8), which indicate our classifier achieves relatively high level
of classification 5. High sensitivity (0.8049) indicates good
identification of positives. 6. High specificity (0.8125) indicates good
identification of negatives. 7. Balanced accuracy (0.8087) suggests the
model balances its performance across both classes well.</p>
</div>
<div id="compute-r-squared-and-rmse-for-the-classification-model"
class="section level3">
<h3>Compute R-squared and RMSE for the classification model</h3>
<pre class="r"><code>rf_pred_numeric = as.numeric(rf_pred2)
test_actual_numeric = as.numeric(testData2$num)

mean_actual = mean(test_actual_numeric)

SST = sum((test_actual_numeric - mean_actual)^2)

SSE = sum((test_actual_numeric - rf_pred_numeric)^2)

R_squared = 1 - (SSE / SST)

# Calculate RMSE
rmse = sqrt(mean((rf_pred_numeric - test_actual_numeric)^2))

# Print the results
cat(&quot;RMSE of the model is:&quot;, rmse, &quot;\n&quot;)</code></pre>
<pre><code>## RMSE of the model is: 0.4364358</code></pre>
<pre class="r"><code>cat(&quot;R-squared: &quot;, R_squared, &quot;\n&quot;)</code></pre>
<pre><code>## R-squared:  0.1996951</code></pre>
<p>Then investigate the AUC value and ROC curve to assess the model’s
ability.</p>
<pre class="r"><code>library(pROC)</code></pre>
<pre><code>## Warning: 程序包&#39;pROC&#39;是用R版本4.4.2 来建造的</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## 载入程序包：&#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre class="r"><code># Generate AUC value
rf_prob = predict(rf_model2, testData2, type = &quot;prob&quot;)
roc_curve = roc(testData2$num, rf_prob[, 2], levels = rev(levels(testData2$num)))</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>auc_value = auc(roc_curve)
roc_data = data.frame(
  FPR = 1 - roc_curve$specificities,
  TPR = roc_curve$sensitivities
)

# Plot the ROC curve
ggplot(data = roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = &quot;blue&quot;, size = 1) +
  geom_abline(linetype = &quot;dashed&quot;, color = &quot;gray&quot;) +
  labs(
    title = &quot;ROC Curve for Random Forest Model&quot;,
    x = &quot;False Positive Rate (1 - Specificity)&quot;,
    y = &quot;True Positive Rate (Sensitivity)&quot;,
    subtitle = paste(&quot;AUC =&quot;, round(auc_value, 2))
  ) +
  theme_minimal()</code></pre>
<pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<p><img src="modelling_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>The AUC value of 0.85 indicates that the Random Forest model performs
well in distinguishing between positive and negative classes.
Specifically, there is an 85% chance that the model will rank a randomly
chosen positive instance higher than a negative one. This reflects our
model reaches good discrimination.</p>
<p>The ROC curve shows the trade-off between the True Positive Rate
(Sensitivity) and the False Positive Rate (1 - Specificity) at various
thresholds. The curve is well above the diagonal which represent random
guessing, confirming the model performs better than random guessing. The
initial steep rise indicates that the model achieves high sensitivity
with a relatively low false positive rate, which is desirable. However,
as the false positive rate increases, the curve flattens, highlighting
diminishing returns in improving sensitivity further.</p>
<p>Last, simulate the prediction which predicts the num with the new
data based on the model we built!</p>
<pre class="r"><code># construct a new dataframe which includes new data
new_data = data.frame(
  age = c(63,39,62,34),
  sex = c(1, 1, 1, 0),
  cp = c(1, 2, 4, 1),
  trestbps = c(145, 120, 110, 125),
  thalach = c(150, 160, 120, 140),
  exang = c(0, 1, 1, 0),
  oldpeak = c(2.3, 1, 0.5, 2),
  slope = c(3, 2, 2, 1),
  region = c(1, 2, 3, 4)
)
# predict the results based on the model we have trained
predicted_num = predict(rf_model2, new_data)
print(&quot;The prediction result is：&quot;)</code></pre>
<pre><code>## [1] &quot;The prediction result is：&quot;</code></pre>
<pre class="r"><code>print(data.frame(new_data, Predicted_num = predicted_num))</code></pre>
<pre><code>##   age sex cp trestbps thalach exang oldpeak slope region Predicted_num
## 1  63   1  1      145     150     0     2.3     3      1             0
## 2  39   1  2      120     160     1     1.0     2      2             0
## 3  62   1  4      110     120     1     0.5     2      3             1
## 4  34   0  1      125     140     0     2.0     1      4             0</code></pre>
<p>From the results, we can see that the model can generate some results
based on the predictor values we put in!</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
